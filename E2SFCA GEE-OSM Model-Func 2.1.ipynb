{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "442b3921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\labib003\\Miniconda3\\envs\\usga\\lib\\site-packages\\osmnx\\projection.py:3: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "# system packages\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# non-geo numeric packages\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import product, combinations\n",
    "import pandas as pd\n",
    "\n",
    "# network and OSM packages\n",
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "city_geo = ox.geocoder.geocode_to_gdf\n",
    "\n",
    "# Earth engine packages\n",
    "import ee\n",
    "import geemap\n",
    "\n",
    "# General geo-packages\n",
    "import libpysal\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "from shapely import geometry\n",
    "from shapely.geometry import Point, MultiLineString, LineString, Polygon, MultiPolygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07277cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=MaDF2Atq4yOk6cvO2QJIHII-z0QqTedo4wDYPhEDj2w&tc=t3_nZAjM7VDUafoDxZEtnU4PvxwfbJJhljoah9BeWrA&cc=suepv7KSYjcgfB7a4FHYDk3EIXR7qv3v8J4BCDouZds>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=MaDF2Atq4yOk6cvO2QJIHII-z0QqTedo4wDYPhEDj2w&tc=t3_nZAjM7VDUafoDxZEtnU4PvxwfbJJhljoah9BeWrA&cc=suepv7KSYjcgfB7a4FHYDk3EIXR7qv3v8J4BCDouZds</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you should paste in the box below.</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter verification code: 4/1AVHEtk6rYRgOENSqeviD8lJjopLS3THIrXkBxIqcrMEmggtNB9ZJJWfM2A4\n",
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "# Authenticate and Initialize Google Earth Engine\n",
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47f4b61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 0 cities and thresholds\n",
    "thresholds = [300, 600, 1000] # route threshold in metres. WHO guideline speaks of access within 300m\n",
    "\n",
    "# Extract iso-3166 country codes\n",
    "iso = pd.read_excel('iso_countries.xlsx')\n",
    "\n",
    "# Extract cities list\n",
    "cities = pd.read_excel('cities.xlsx') # all cities\n",
    "\n",
    "# 'cities_adj' serves by default as city-input for functions\n",
    "# cities_adj = cities\n",
    "# cities_adj = cities[cities['Included (Y/N)'] == 'Y']\n",
    "cities_adj = cities[cities['City'].isin(['Addis Ababa','Dhaka','Shijiazhuang','Damascus'])]\n",
    "cities_adj = cities_adj.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa5e18f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/f0de31b02bb07953d66a210523e93d01-7b0b087c4f609d6f8c70b5e9ddc2eeb6:getPixels\n",
      "Please wait ...\n",
      "Data downloaded to C:\\Dumps\\GEE_city_grids\\ETH_Addis Ababa_2020.tif\n",
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/ff61bd25735c10bedb131185a12a5a39-008b6a660de11e8d5aaebdd2f0cc1fbc:getPixels\n",
      "Please wait ...\n",
      "Data downloaded to C:\\Dumps\\GEE_city_grids\\SYR_Damascus_2020.tif\n",
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/4515b4f8219995aafd94d6b7bd809e29-5a24aeaed12f37765884e6bfa934a551:getPixels\n",
      "Please wait ...\n",
      "Data downloaded to C:\\Dumps\\GEE_city_grids\\BGD_Dhaka_2020.tif\n",
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/c00d2dec9576c7941a5faf8cf3e6eb04-33f897acb3b8c2b1f2070b83489f95bd:getPixels\n",
      "Please wait ...\n",
      "Data downloaded to C:\\Dumps\\GEE_city_grids\\CHN_Shijiazhuang_2020.tif\n",
      "CPU times: total: 2.25 s\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 1. Required preprocess for information extraction\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# In essence, we use Google Earth Engine to extract a country's grid raster and clip it with the city's preferred OSM area\n",
    "# Predifine in Excel: the (1) city name as \"City\" and (2) the OSM area that needs to be extracted as \"OSM_area\"\n",
    "# i.e. City = \"Los Angeles\" and OSM_area = \"Los Angeles county, Orange county CA\"\n",
    "files = gee_worldpop_extract(cities_adj, iso, 'C:/Dumps/GEE_city_grids/')\n",
    "\n",
    "# Files are downloaded automatically to the specified path. Files are also stored in Google with a downloadlink:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df72d9e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100m resolution grids extraction\n",
      "Addis Ababa 0.35 mns\n",
      "Damascus 0.45 mns\n",
      "Dhaka 0.65 mns\n",
      "Shijiazhuang 0.95 mns\n",
      " \n",
      "get road networks from OSM\n",
      "Addis Ababa done 1.01 mns\n",
      "Damascus done 1.31 mns\n",
      "Dhaka done 1.93 mns\n",
      "Shijiazhuang done 2.13 mns\n",
      " \n",
      "get urban greenspaces from OSM\n",
      "Addis Ababa done\n",
      "Damascus done\n",
      "Dhaka done\n",
      "Shijiazhuang done\n",
      "CPU times: total: 3min 8s\n",
      "Wall time: 3min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 2. Information extraction\n",
    "\n",
    "# Clip cities from countries, format population grids\n",
    "population_grids = city_grids_format(files,\n",
    "                                     grid_size = 100) # aggregating upwards to i.e. 200m, 300m etc. is possible\n",
    "print(' ')\n",
    "\n",
    "# Get road networks\n",
    "road_networks = road_networks(cities_adj, # Get 'all' (drive,walk,bike) network\n",
    "                              thresholds,\n",
    "                              undirected = True)\n",
    "print(' ')\n",
    "\n",
    "# Extract urban greenspace (UGS)\n",
    "UGS = urban_greenspace(cities_adj, \n",
    "                       thresholds,\n",
    "                       one_UGS_buf = 25, # buffer at which UGS is seen as one\n",
    "                       min_UGS_size = 400) # WHO sees this as minimum UGS size (400m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a673017",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get fake UGS entry points\n",
      "Addis Ababa 0.0 % done 0.01  mns\n",
      "Addis Ababa 73.5 % done 0.29  mns\n",
      "Addis Ababa 100 % done 0.4  mns\n",
      "Damascus 0.0 % done 0.4  mns\n",
      "Damascus 39.8 % done 0.51  mns\n",
      "Damascus 79.7 % done 0.61  mns\n",
      "Damascus 100 % done 0.67  mns\n",
      "Dhaka 0.0 % done 0.68  mns\n",
      "Dhaka 19.0 % done 0.85  mns\n",
      "Dhaka 38.1 % done 1.01  mns\n",
      "Dhaka 57.1 % done 1.18  mns\n",
      "Dhaka 76.2 % done 1.34  mns\n",
      "Dhaka 95.2 % done 1.51  mns\n",
      "Dhaka 100 % done 1.55  mns\n",
      "Shijiazhuang 0.0 % done 1.55  mns\n",
      "Shijiazhuang 100 % done 1.62  mns\n",
      " \n",
      "get potential (Euclidean) suitible combinations\n",
      "Addis Ababa\n",
      "in chunk 1 / 10 91206 suitible comb.\n",
      "in chunk 2 / 10 6658 suitible comb.\n",
      "in chunk 3 / 10 9533 suitible comb.\n",
      "in chunk 4 / 10 1465 suitible comb.\n",
      "in chunk 5 / 10 1809 suitible comb.\n",
      "in chunk 6 / 10 2842 suitible comb.\n",
      "in chunk 7 / 10 3583 suitible comb.\n",
      "in chunk 8 / 10 14099 suitible comb.\n",
      "in chunk 9 / 10 27021 suitible comb.\n",
      "in chunk 10 / 10 50084 suitible comb.\n",
      "total combinations within distance 208300\n",
      "0.0 % gridentry done 0.0  mns\n",
      "100 % gridentry done 2.73  mns\n",
      "Damascus\n",
      "in chunk 1 / 5 18430 suitible comb.\n",
      "in chunk 2 / 5 41027 suitible comb.\n",
      "in chunk 3 / 5 84548 suitible comb.\n",
      "in chunk 4 / 5 175197 suitible comb.\n",
      "in chunk 5 / 5 114367 suitible comb.\n",
      "total combinations within distance 433569\n",
      "0.0 % gridentry done 0.0  mns\n",
      "57.7 % gridentry done 0.51  mns\n",
      "100 % gridentry done 5.14  mns\n",
      "Dhaka\n",
      "in chunk 1 / 5 12953 suitible comb.\n",
      "in chunk 2 / 5 15622 suitible comb.\n",
      "in chunk 3 / 5 10007 suitible comb.\n",
      "in chunk 4 / 5 1732 suitible comb.\n",
      "in chunk 5 / 5 5180 suitible comb.\n",
      "total combinations within distance 45494\n",
      "0.0 % gridentry done 0.0  mns\n",
      "100 % gridentry done 6.2  mns\n",
      "Shijiazhuang\n",
      "in chunk 1 / 3 8013 suitible comb.\n",
      "in chunk 2 / 3 24802 suitible comb.\n",
      "in chunk 3 / 3 9845 suitible comb.\n",
      "total combinations within distance 42660\n",
      "0.0 % gridentry done 0.0  mns\n",
      "100 % gridentry done 6.82  mns\n",
      " \n",
      "Check grids within UGS\n",
      "0 0.02  mns\n",
      "100 0.69  mns\n",
      "Check grids within UGS\n",
      "0 0.93  mns\n",
      "100 1.1  mns\n",
      "200 1.26  mns\n",
      "Check grids within UGS\n",
      "0 1.35  mns\n",
      "100 1.63  mns\n",
      "200 1.89  mns\n",
      "300 2.15  mns\n",
      "400 2.41  mns\n",
      "500 2.67  mns\n",
      "Check grids within UGS\n",
      "0 2.78  mns\n",
      "CPU times: total: 11min 45s\n",
      "Wall time: 11min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 3. Preprocess information for route finding\n",
    "\n",
    "# Get fake entry points (between UGS and buffer limits)\n",
    "UGS_entry = UGS_fake_entry(UGS, \n",
    "                           road_networks['nodes'], \n",
    "                           cities_adj['City'],\n",
    "                           UGS_entry_buf = 25, # road nodes within 25 meters are seen as fake entry points\n",
    "                           walk_radius = 500, # assume that the average person only views a UGS up to 500m in radius\n",
    "                                                # more attractive\n",
    "                           entry_point_merge = 0) # merges closeby fake UGS entry points within X meters \n",
    "                                                    # what may be done for performance\n",
    "print(' ')\n",
    "# Checks all potential suitible combinations (points that fall within max threshold Euclidean distance from the ego)\n",
    "suitible = suitible_combinations(UGS_entry, \n",
    "                                 population_grids, \n",
    "                                 road_networks['nodes'], # For finding nearest grid entry points\n",
    "                                 thresholds,\n",
    "                                 cities_adj['City'],\n",
    "                                 chunk_size = 10000000) # calculating per chunk of num UGS entry points * num pop_grids\n",
    "                                                        # Preventing normal PC meltdown, set lower if PC gets stuck\n",
    "print(' ')\n",
    "# Checks if grids are already in a UGS\n",
    "suitible_InOut_UGS = grids_in_UGS (suitible, UGS, population_grids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b98ff692",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addis Ababa 1 / 1 range 0 - 176877\n",
      "0.0 % done 0.0 mns\n",
      "5.65 % done 0.07 mns\n",
      "11.31 % done 0.14 mns\n",
      "16.96 % done 0.23 mns\n",
      "22.61 % done 0.31 mns\n",
      "28.27 % done 0.42 mns\n",
      "33.92 % done 0.53 mns\n",
      "39.58 % done 0.69 mns\n",
      "45.23 % done 0.79 mns\n",
      "50.88 % done 0.87 mns\n",
      "56.54 % done 0.95 mns\n",
      "index 102412 No route\n",
      "index 102413 No route\n",
      "index 102414 No route\n",
      "index 102415 No route\n",
      "index 102511 No route\n",
      "index 102512 No route\n",
      "index 102514 No route\n",
      "index 102515 No route\n",
      "index 102516 No route\n",
      "62.19 % done 1.35 mns\n",
      "67.84 % done 1.4 mns\n",
      "73.5 % done 1.46 mns\n",
      "79.15 % done 1.51 mns\n",
      "84.8 % done 1.58 mns\n",
      "90.46 % done 1.65 mns\n",
      "96.11 % done 1.74 mns\n",
      "for 32 routes nearest nodes found\n",
      "100.0 % pathfinding done 1.79 mns\n",
      "formatting done 2.68 mns\n",
      "dissolving done 3.39 mns\n",
      "Damascus 1 / 2 range 0 - 250000\n",
      "0.0 % done 3.4 mns\n",
      "2.35 % done 7.47 mns\n",
      "4.7 % done 8.06 mns\n",
      "7.05 % done 8.16 mns\n",
      "9.4 % done 8.28 mns\n",
      "11.75 % done 8.36 mns\n",
      "14.1 % done 8.44 mns\n",
      "16.45 % done 8.5 mns\n",
      "18.8 % done 8.64 mns\n",
      "21.15 % done 8.74 mns\n",
      "23.49 % done 8.9 mns\n",
      "25.84 % done 8.99 mns\n",
      "28.19 % done 9.12 mns\n",
      "30.54 % done 9.19 mns\n",
      "32.89 % done 9.31 mns\n",
      "35.24 % done 9.4 mns\n",
      "37.59 % done 9.46 mns\n",
      "39.94 % done 9.53 mns\n",
      "42.29 % done 9.59 mns\n",
      "44.64 % done 9.68 mns\n",
      "46.99 % done 9.75 mns\n",
      "49.34 % done 9.84 mns\n",
      "51.69 % done 9.94 mns\n",
      "54.04 % done 10.03 mns\n",
      "56.39 % done 10.1 mns\n",
      "for 1685 routes nearest nodes found\n",
      "58.74 % pathfinding done 10.18 mns\n",
      "formatting done 11.52 mns\n",
      "dissolving done 12.56 mns\n",
      "Damascus 2 / 2 range 250000 - 425623\n",
      "58.74 % done 12.57 mns\n",
      "61.09 % done 12.66 mns\n",
      "63.44 % done 12.87 mns\n",
      "65.79 % done 12.94 mns\n",
      "68.14 % done 13.01 mns\n",
      "70.48 % done 13.08 mns\n",
      "72.83 % done 13.17 mns\n",
      "index 314211 No route\n",
      "index 314212 No route\n",
      "index 314213 No route\n",
      "index 314214 No route\n",
      "index 314215 No route\n",
      "index 314216 No route\n",
      "index 314217 No route\n",
      "index 314218 No route\n",
      "index 314219 No route\n",
      "index 314220 No route\n",
      "index 314221 No route\n",
      "index 314222 No route\n",
      "index 314223 No route\n",
      "index 314224 No route\n",
      "index 314225 No route\n",
      "index 314226 No route\n",
      "index 314227 No route\n",
      "index 314228 No route\n",
      "index 314229 No route\n",
      "index 314230 No route\n",
      "index 314231 No route\n",
      "index 314232 No route\n",
      "index 314233 No route\n",
      "index 314234 No route\n",
      "index 314235 No route\n",
      "index 314236 No route\n",
      "index 314237 No route\n",
      "index 314238 No route\n",
      "index 314239 No route\n",
      "index 314240 No route\n",
      "index 314241 No route\n",
      "index 314242 No route\n",
      "index 314243 No route\n",
      "index 314244 No route\n",
      "index 314245 No route\n",
      "index 314246 No route\n",
      "index 314247 No route\n",
      "index 314248 No route\n",
      "index 314249 No route\n",
      "index 314250 No route\n",
      "index 314251 No route\n",
      "index 314252 No route\n",
      "index 314253 No route\n",
      "index 314254 No route\n",
      "index 314255 No route\n",
      "index 314256 No route\n",
      "index 314257 No route\n",
      "index 314258 No route\n",
      "index 314259 No route\n",
      "index 314260 No route\n",
      "index 314261 No route\n",
      "index 314262 No route\n",
      "index 314263 No route\n",
      "index 314264 No route\n",
      "index 314265 No route\n",
      "index 314266 No route\n",
      "index 314267 No route\n",
      "index 314268 No route\n",
      "index 314269 No route\n",
      "index 314270 No route\n",
      "index 314271 No route\n",
      "index 314272 No route\n",
      "index 314273 No route\n",
      "index 314274 No route\n",
      "75.18 % done 13.62 mns\n",
      "77.53 % done 13.82 mns\n",
      "79.88 % done 14.06 mns\n",
      "82.23 % done 14.15 mns\n",
      "index 357168 No route\n",
      "index 357169 No route\n",
      "index 357170 No route\n",
      "index 357171 No route\n",
      "index 357172 No route\n",
      "index 357173 No route\n",
      "index 357174 No route\n",
      "index 357175 No route\n",
      "index 357176 No route\n",
      "index 357177 No route\n",
      "index 357218 No route\n",
      "index 357219 No route\n",
      "index 357220 No route\n",
      "index 357221 No route\n",
      "index 357222 No route\n",
      "index 357223 No route\n",
      "index 357224 No route\n",
      "index 357225 No route\n",
      "index 357226 No route\n",
      "index 357227 No route\n",
      "index 357228 No route\n",
      "index 357229 No route\n",
      "index 357230 No route\n",
      "index 357231 No route\n",
      "index 357232 No route\n",
      "index 357233 No route\n",
      "index 357234 No route\n",
      "index 357235 No route\n",
      "index 357236 No route\n",
      "index 357237 No route\n",
      "index 357238 No route\n",
      "index 357239 No route\n",
      "index 357240 No route\n",
      "index 357241 No route\n",
      "index 357242 No route\n",
      "index 357243 No route\n",
      "index 357244 No route\n",
      "index 357245 No route\n",
      "index 357246 No route\n",
      "index 357247 No route\n",
      "index 357248 No route\n",
      "index 357249 No route\n",
      "index 357250 No route\n",
      "index 357251 No route\n",
      "index 357252 No route\n",
      "index 357253 No route\n",
      "index 357254 No route\n",
      "index 357255 No route\n",
      "index 357256 No route\n",
      "index 357257 No route\n",
      "index 357258 No route\n",
      "index 357259 No route\n",
      "index 357260 No route\n",
      "index 357261 No route\n",
      "index 357262 No route\n",
      "index 357263 No route\n",
      "index 357264 No route\n",
      "index 357265 No route\n",
      "index 357266 No route\n",
      "index 357267 No route\n",
      "index 357268 No route\n",
      "index 357269 No route\n",
      "index 357270 No route\n",
      "index 357271 No route\n",
      "index 357272 No route\n",
      "index 357273 No route\n",
      "index 357274 No route\n",
      "index 357275 No route\n",
      "index 357276 No route\n",
      "index 357277 No route\n",
      "index 357278 No route\n",
      "index 357279 No route\n",
      "index 357280 No route\n",
      "index 357281 No route\n",
      "index 357282 No route\n",
      "index 357283 No route\n",
      "index 357284 No route\n",
      "index 357285 No route\n",
      "index 357286 No route\n",
      "index 357287 No route\n",
      "index 357288 No route\n",
      "index 357289 No route\n",
      "index 357290 No route\n",
      "index 357291 No route\n",
      "index 357292 No route\n",
      "index 357293 No route\n",
      "index 357294 No route\n",
      "index 357295 No route\n",
      "index 357296 No route\n",
      "index 357297 No route\n",
      "index 357298 No route\n",
      "index 357299 No route\n",
      "index 357300 No route\n",
      "index 357301 No route\n",
      "index 357302 No route\n",
      "index 357313 No route\n",
      "index 357314 No route\n",
      "index 357315 No route\n",
      "index 357316 No route\n",
      "index 357317 No route\n",
      "index 357318 No route\n",
      "index 357319 No route\n",
      "index 357320 No route\n",
      "index 357321 No route\n",
      "index 357322 No route\n",
      "index 357323 No route\n",
      "index 357324 No route\n",
      "index 357325 No route\n",
      "index 357326 No route\n",
      "index 357327 No route\n",
      "index 357328 No route\n",
      "index 357329 No route\n",
      "index 357330 No route\n",
      "index 357331 No route\n",
      "index 357332 No route\n",
      "index 357333 No route\n",
      "index 357334 No route\n",
      "index 357335 No route\n",
      "index 357336 No route\n",
      "index 357337 No route\n",
      "index 357338 No route\n",
      "index 357359 No route\n",
      "index 357360 No route\n",
      "index 357361 No route\n",
      "index 357362 No route\n",
      "index 357363 No route\n",
      "index 357364 No route\n",
      "index 357365 No route\n",
      "index 357366 No route\n",
      "index 357367 No route\n",
      "index 357368 No route\n",
      "index 357369 No route\n",
      "index 357370 No route\n",
      "index 357371 No route\n",
      "index 357372 No route\n",
      "index 357373 No route\n",
      "index 357374 No route\n",
      "index 357375 No route\n",
      "index 357376 No route\n",
      "index 357377 No route\n",
      "index 357378 No route\n",
      "index 357379 No route\n",
      "index 357380 No route\n",
      "index 357381 No route\n",
      "index 357382 No route\n",
      "index 357383 No route\n",
      "index 357384 No route\n",
      "index 357385 No route\n",
      "index 357386 No route\n",
      "index 357387 No route\n",
      "index 357388 No route\n",
      "index 357389 No route\n",
      "index 357390 No route\n",
      "index 357391 No route\n",
      "index 357392 No route\n",
      "index 357393 No route\n",
      "index 357394 No route\n",
      "index 357395 No route\n",
      "index 357396 No route\n",
      "index 357397 No route\n",
      "index 357398 No route\n",
      "index 357399 No route\n",
      "index 357400 No route\n",
      "index 357401 No route\n",
      "index 357402 No route\n",
      "84.58 % done 17.48 mns\n",
      "index 365232 No route\n",
      "index 365233 No route\n",
      "index 365234 No route\n",
      "index 365235 No route\n",
      "index 365236 No route\n",
      "index 365237 No route\n",
      "index 365238 No route\n",
      "index 365239 No route\n",
      "index 365240 No route\n",
      "index 365241 No route\n",
      "index 365242 No route\n",
      "index 365243 No route\n",
      "index 365244 No route\n",
      "index 365245 No route\n",
      "index 365246 No route\n",
      "index 365247 No route\n",
      "index 365248 No route\n",
      "index 365249 No route\n",
      "index 365250 No route\n",
      "index 365251 No route\n",
      "index 365252 No route\n",
      "index 365253 No route\n",
      "index 365366 No route\n",
      "index 365367 No route\n",
      "index 365374 No route\n",
      "index 365375 No route\n",
      "index 365376 No route\n",
      "index 365377 No route\n",
      "index 365378 No route\n",
      "index 365379 No route\n",
      "index 365380 No route\n",
      "index 365381 No route\n",
      "index 365382 No route\n",
      "index 365383 No route\n",
      "index 365384 No route\n",
      "index 365385 No route\n",
      "index 365386 No route\n",
      "index 365387 No route\n",
      "index 365388 No route\n",
      "index 365389 No route\n",
      "index 365390 No route\n",
      "index 365391 No route\n",
      "index 365392 No route\n",
      "index 365393 No route\n",
      "index 365394 No route\n",
      "index 365395 No route\n",
      "index 365396 No route\n",
      "index 365397 No route\n",
      "index 365398 No route\n",
      "index 365399 No route\n",
      "index 365400 No route\n",
      "index 365401 No route\n",
      "index 365402 No route\n",
      "index 365403 No route\n",
      "index 365404 No route\n",
      "index 365405 No route\n",
      "index 365406 No route\n",
      "index 365407 No route\n",
      "index 365408 No route\n",
      "index 365409 No route\n",
      "index 365410 No route\n",
      "index 365411 No route\n",
      "index 365412 No route\n",
      "index 365413 No route\n",
      "index 365414 No route\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 365415 No route\n",
      "index 365490 No route\n",
      "index 365491 No route\n",
      "86.93 % done 21.88 mns\n",
      "89.28 % done 21.93 mns\n",
      "91.63 % done 22.0 mns\n",
      "93.98 % done 22.39 mns\n",
      "96.33 % done 22.46 mns\n",
      "98.68 % done 22.57 mns\n",
      "index 425555 No route\n",
      "index 425556 No route\n",
      "index 425557 No route\n",
      "index 425558 No route\n",
      "index 425559 No route\n",
      "index 425560 No route\n",
      "index 425561 No route\n",
      "index 425562 No route\n",
      "index 425563 No route\n",
      "index 425564 No route\n",
      "index 425565 No route\n",
      "index 425566 No route\n",
      "index 425567 No route\n",
      "index 425568 No route\n",
      "index 425569 No route\n",
      "index 425570 No route\n",
      "index 425571 No route\n",
      "index 425572 No route\n",
      "index 425573 No route\n",
      "index 425574 No route\n",
      "index 425575 No route\n",
      "index 425576 No route\n",
      "index 425577 No route\n",
      "index 425578 No route\n",
      "index 425579 No route\n",
      "index 425580 No route\n",
      "index 425581 No route\n",
      "index 425582 No route\n",
      "index 425583 No route\n",
      "index 425584 No route\n",
      "index 425585 No route\n",
      "index 425586 No route\n",
      "index 425587 No route\n",
      "index 425588 No route\n",
      "index 425589 No route\n",
      "index 425590 No route\n",
      "index 425591 No route\n",
      "index 425592 No route\n",
      "index 425593 No route\n",
      "index 425594 No route\n",
      "index 425595 No route\n",
      "index 425596 No route\n",
      "index 425597 No route\n",
      "index 425598 No route\n",
      "index 425599 No route\n",
      "index 425600 No route\n",
      "index 425601 No route\n",
      "index 425602 No route\n",
      "index 425603 No route\n",
      "index 425604 No route\n",
      "index 425605 No route\n",
      "index 425606 No route\n",
      "index 425607 No route\n",
      "index 425608 No route\n",
      "index 425609 No route\n",
      "index 425610 No route\n",
      "index 425611 No route\n",
      "index 425612 No route\n",
      "for 3767 routes nearest nodes found\n",
      "100.0 % pathfinding done 23.23 mns\n",
      "formatting done 24.07 mns\n",
      "dissolving done 24.78 mns\n",
      "Dhaka 1 / 1 range 0 - 43937\n",
      "0.0 % done 24.8 mns\n",
      "22.76 % done 24.83 mns\n",
      "45.52 % done 24.85 mns\n",
      "68.28 % done 24.88 mns\n",
      "91.04 % done 24.92 mns\n",
      "for 0 routes nearest nodes found\n",
      "100.0 % pathfinding done 24.93 mns\n",
      "formatting done 25.06 mns\n",
      "dissolving done 25.22 mns\n",
      "Shijiazhuang 1 / 1 range 0 - 38356\n",
      "0.0 % done 25.22 mns\n",
      "26.07 % done 25.24 mns\n",
      "52.14 % done 25.27 mns\n",
      "78.21 % done 25.3 mns\n",
      "for 234 routes nearest nodes found\n",
      "100.0 % pathfinding done 25.59 mns\n",
      "formatting done 25.68 mns\n",
      "dissolving done 25.81 mns\n",
      "CPU times: total: 25min 49s\n",
      "Wall time: 25min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 4. Finding shortest routes.\n",
    "Routes = route_finding (road_networks['graphs'], # graphs of the road networks\n",
    "               suitible_InOut_UGS, # potential suitible routes with grid-UGS comb. separated in or out UGS.\n",
    "               road_networks['nodes'], \n",
    "               road_networks['edges'], \n",
    "               cities_adj['City'], \n",
    "               block_size = 250000, # Chunk to spread dataload.\n",
    "               nn_iter = 10) # max amount of nearest nodes to be found (both for UGS entry and grid-centroid road entries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a49890ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 Addis Ababa\n",
      "600 Addis Ababa\n",
      "1000 Addis Ababa\n",
      "300 Damascus\n",
      "600 Damascus\n",
      "1000 Damascus\n",
      "300 Dhaka\n",
      "600 Dhaka\n",
      "1000 Dhaka\n",
      "300 Shijiazhuang\n",
      "600 Shijiazhuang\n",
      "1000 Shijiazhuang\n",
      "CPU times: total: 57.7 s\n",
      "Wall time: 57.9 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>City</th>\n",
       "      <th>Addis Ababa</th>\n",
       "      <th>Damascus</th>\n",
       "      <th>Dhaka</th>\n",
       "      <th>Shijiazhuang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>population</th>\n",
       "      <td>3,803,260.00</td>\n",
       "      <td>2,710,137.00</td>\n",
       "      <td>13,773,976.00</td>\n",
       "      <td>3,342,673.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sc-access 300</th>\n",
       "      <td>64.19</td>\n",
       "      <td>0.86</td>\n",
       "      <td>2.53</td>\n",
       "      <td>22.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-dist 300</th>\n",
       "      <td>4.61</td>\n",
       "      <td>27.21</td>\n",
       "      <td>10.19</td>\n",
       "      <td>3.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-area 300</th>\n",
       "      <td>6,889,229.79</td>\n",
       "      <td>9,544.27</td>\n",
       "      <td>24,081.83</td>\n",
       "      <td>32,507.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-supply 300</th>\n",
       "      <td>65.84</td>\n",
       "      <td>0.72</td>\n",
       "      <td>2.78</td>\n",
       "      <td>24.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sc-norm 300</th>\n",
       "      <td>8.86</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.15</td>\n",
       "      <td>5.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sc-access 600</th>\n",
       "      <td>57.23</td>\n",
       "      <td>0.86</td>\n",
       "      <td>2.11</td>\n",
       "      <td>27.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-dist 600</th>\n",
       "      <td>20.19</td>\n",
       "      <td>88.68</td>\n",
       "      <td>46.94</td>\n",
       "      <td>14.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-area 600</th>\n",
       "      <td>7,858,433.07</td>\n",
       "      <td>11,932.18</td>\n",
       "      <td>37,891.72</td>\n",
       "      <td>53,493.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-supply 600</th>\n",
       "      <td>59.37</td>\n",
       "      <td>0.59</td>\n",
       "      <td>2.22</td>\n",
       "      <td>34.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sc-norm 600</th>\n",
       "      <td>7.79</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.09</td>\n",
       "      <td>7.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sc-access 1000</th>\n",
       "      <td>49.09</td>\n",
       "      <td>0.87</td>\n",
       "      <td>2.02</td>\n",
       "      <td>26.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-dist 1000</th>\n",
       "      <td>56.12</td>\n",
       "      <td>204.91</td>\n",
       "      <td>86.19</td>\n",
       "      <td>37.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-area 1000</th>\n",
       "      <td>9,082,470.86</td>\n",
       "      <td>13,874.22</td>\n",
       "      <td>47,011.38</td>\n",
       "      <td>75,730.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-supply 1000</th>\n",
       "      <td>50.91</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.92</td>\n",
       "      <td>31.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sc-norm 1000</th>\n",
       "      <td>6.12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.10</td>\n",
       "      <td>6.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "City                    Addis Ababa             Damascus                Dhaka  \\\n",
       "population             3,803,260.00         2,710,137.00        13,773,976.00   \n",
       "Sc-access 300                 64.19                 0.86                 2.53   \n",
       "M-dist 300                     4.61                27.21                10.19   \n",
       "M-area 300             6,889,229.79             9,544.27            24,081.83   \n",
       "M-supply 300                  65.84                 0.72                 2.78   \n",
       "Sc-norm 300                    8.86                 0.01                 0.15   \n",
       "Sc-access 600                 57.23                 0.86                 2.11   \n",
       "M-dist 600                    20.19                88.68                46.94   \n",
       "M-area 600             7,858,433.07            11,932.18            37,891.72   \n",
       "M-supply 600                  59.37                 0.59                 2.22   \n",
       "Sc-norm 600                    7.79                 0.01                 0.09   \n",
       "Sc-access 1000                49.09                 0.87                 2.02   \n",
       "M-dist 1000                   56.12               204.91                86.19   \n",
       "M-area 1000            9,082,470.86            13,874.22            47,011.38   \n",
       "M-supply 1000                 50.91                 0.50                 1.92   \n",
       "Sc-norm 1000                   6.12                 0.01                 0.10   \n",
       "\n",
       "City                   Shijiazhuang  \n",
       "population             3,342,673.00  \n",
       "Sc-access 300                 22.86  \n",
       "M-dist 300                     3.51  \n",
       "M-area 300                32,507.79  \n",
       "M-supply 300                  24.54  \n",
       "Sc-norm 300                    5.41  \n",
       "Sc-access 600                 27.69  \n",
       "M-dist 600                    14.61  \n",
       "M-area 600                53,493.05  \n",
       "M-supply 600                  34.02  \n",
       "Sc-norm 600                    7.26  \n",
       "Sc-access 1000                26.38  \n",
       "M-dist 1000                   37.14  \n",
       "M-area 1000               75,730.96  \n",
       "M-supply 1000                 31.17  \n",
       "Sc-norm 1000                   6.62  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 5. summarize scores\n",
    "min_gridUGS = min_gridUGS_comb (Routes, population_grids, UGS)\n",
    "\n",
    "E2SCFA_score = E2SCFA_scores(min_gridUGS, \n",
    "                             population_grids, \n",
    "                             thresholds, \n",
    "                             cities_adj['City'], \n",
    "                             save_path = 'C:/Users/labib003/UGSA/Dumps/GEE-WP Scores/E2SFCA/', \n",
    "                             grid_size = 100)\n",
    "\n",
    "E2SCFA_score['score summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "313b0bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gee_worldpop_extract (city_file, iso, save_path = None):\n",
    "    \n",
    "    cities = city_file\n",
    "    \n",
    "    # Get included city areas\n",
    "    OSM_incl = [cities[cities['City'] == city]['OSM_area'].tolist()[0].rsplit(', ') for city in cities['City'].tolist()]\n",
    "\n",
    "    # Get the city geoms\n",
    "    obj = [city_geo(city).dissolve()['geometry'].tolist()[0] for city in OSM_incl]\n",
    "\n",
    "    # Get the city countries\n",
    "    obj_displ = [city_geo(city).dissolve()['display_name'].tolist()[0].rsplit(', ')[-1]for city in OSM_incl]\n",
    "    obj_displ = np.where(pd.Series(obj_displ).str.contains(\"Ivoire\"),\"CIte dIvoire\",obj_displ)\n",
    "\n",
    "    # Get the country's iso-code\n",
    "    iso_list = [iso[iso['name'] == ob]['alpha3'].tolist()[0] for ob in obj_displ]\n",
    "\n",
    "    # Based on the iso-code return the worldpop 2020\n",
    "    ee_worldpop = [ee.ImageCollection(\"WorldPop/GP/100m/pop\")\\\n",
    "        .filter(ee.Filter.date('2020'))\\\n",
    "        .filter(ee.Filter.inList('country', [io])).first() for io in iso_list]\n",
    "\n",
    "    # Clip the countries with the city geoms.\n",
    "    clipped = [ee_worldpop[i].clip(shapely.geometry.mapping(obj[i])) for i in range(0,len(obj))]\n",
    "\n",
    "    # Create path if non-existent\n",
    "    if save_path == None:\n",
    "        path = ''\n",
    "    else:\n",
    "        path = save_path\n",
    "        if not os.path.exists(path):\n",
    "                    os.makedirs(path)\n",
    "\n",
    "    # Export as TIFF file.\n",
    "    # Stored in form path + USA_Los Angeles_2020.tif\n",
    "    filenames = [path+iso_list[i]+'_'+cities['City'][i]+'_2020.tif' for i in range(len(obj))]\n",
    "    [geemap.ee_export_image(clipped[i], filename = filenames[i]) for i in range(0,len(obj))]\n",
    "    return(filenames)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e5a4c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2 population grids extraction\n",
    "def city_grids_format(city_grids, grid_size = 100):\n",
    "    start_time = time.time()\n",
    "    grids = []\n",
    "    print(str(grid_size) + 'm resolution grids extraction')\n",
    "    for i in range(len(city_grids)):\n",
    "        \n",
    "        # Open the raster file\n",
    "        with rasterio.open(city_grids[i]) as src:\n",
    "            band= src.read() # the population values\n",
    "            aff = src.transform # the raster bounds and size (affine)\n",
    "        \n",
    "        # Get the rowwise arrays, get a 2D dataframe\n",
    "        grid = pd.DataFrame()\n",
    "        for b in enumerate(band[0]):\n",
    "            grid = pd.concat([grid, pd.Series(b[1],name=b[0])],axis=1)\n",
    "        grid= grid.unstack().reset_index() \n",
    "        \n",
    "        # Unstack df to columns\n",
    "        grid.columns = ['row','col','value']\n",
    "        grid['minx'] = aff[2]+aff[0]*grid['col']\n",
    "        grid['miny'] = aff[5]+aff[4]*grid['row']\n",
    "        grid['maxx'] = aff[2]+aff[0]*grid['col']+aff[0]\n",
    "        grid['maxy'] = aff[5]+aff[4]*grid['row']+aff[4]\n",
    "        \n",
    "        # Create polygon from affine bounds and row/col indices\n",
    "        grid['geometry'] = [Polygon([(grid.minx[i],grid.miny[i]),\n",
    "                                   (grid.maxx[i],grid.miny[i]),\n",
    "                                   (grid.maxx[i],grid.maxy[i]),\n",
    "                                   (grid.minx[i],grid.maxy[i])])\\\n",
    "                          for i in range(len(grid))]\n",
    "        \n",
    "        # Set the df as geo-df\n",
    "        grid = gpd.GeoDataFrame(grid, crs = 4326) \n",
    "\n",
    "        # Get dissolvement_key for dissolvement. \n",
    "        grid['row3'] = np.floor(grid['row']/(grid_size/100)).astype(int)\n",
    "        grid['col3'] = np.floor(grid['col']/(grid_size/100)).astype(int)\n",
    "        grid['dissolve_key'] = grid['row3'].astype(str) +'-'+ grid['col3'].astype(str)\n",
    "\n",
    "        # Dissolve into block by block grids\n",
    "        popgrid = grid[['dissolve_key','geometry','row3','col3']].dissolve('dissolve_key')\n",
    "\n",
    "        # Get those grids populations and area. Only blocks with population and full blocks\n",
    "        popgrid['population'] = round(grid.groupby('dissolve_key')['value'].sum()).astype(int)\n",
    "        popgrid['area_m'] = round(gpd.GeoSeries(popgrid['geometry'], crs = 4326).to_crs(3043).area).astype(int)\n",
    "        popgrid = popgrid[popgrid['population'] > 0]\n",
    "        popgrid = popgrid[popgrid['area_m'] / popgrid['area_m'].max() > 0.95]\n",
    "\n",
    "        # Get centroids and coords\n",
    "        popgrid['centroid'] = popgrid['geometry'].centroid\n",
    "        popgrid['centroid_m'] = gpd.GeoSeries(popgrid['centroid'], crs = 4326).to_crs(3043)\n",
    "        popgrid['grid_lon'] = popgrid['centroid_m'].x\n",
    "        popgrid['grid_lat'] = popgrid['centroid_m'].y\n",
    "        popgrid = popgrid.reset_index()\n",
    "\n",
    "        minx = popgrid.bounds['minx']\n",
    "        maxx = popgrid.bounds['maxx']\n",
    "        miny = popgrid.bounds['miny']\n",
    "        maxy = popgrid.bounds['maxy']\n",
    "\n",
    "        # Some geometries result in a multipolygon when dissolving (like i.e. 0.05 meters), coords error.\n",
    "        # Therefore recreate the polygon.\n",
    "        Poly = []\n",
    "        for k in range(len(popgrid)):\n",
    "            Poly.append(Polygon([(minx[k],maxy[k]),(maxx[k],maxy[k]),(maxx[k],miny[k]),(minx[k],miny[k])]))\n",
    "        popgrid['geometry'] = Poly\n",
    "\n",
    "        grids.append(popgrid)\n",
    "\n",
    "        print(city_grids[i].rsplit('_')[3], round((time.time() - start_time)/60,2),'mns')\n",
    "    return(grids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a8007ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3 Road networks\n",
    "def road_networks (cities, thresholds, undirected = False):\n",
    "    print('get road networks from OSM')\n",
    "    start_time = time.time()\n",
    "    graphs = list()\n",
    "    road_nodes = list()\n",
    "    road_edges = list()\n",
    "    road_conn = list()\n",
    "\n",
    "    for i in enumerate(cities['OSM_area']):\n",
    "        # Get graph, road nodes and edges\n",
    "        road_node = pd.DataFrame()\n",
    "        roads = pd.DataFrame()\n",
    "        \n",
    "        # For each included OSM_area get the roads\n",
    "        for district in i[1].rsplit(', '):\n",
    "            graph = ox.graph_from_place(district, network_type = \"all\", buffer_dist = (np.max(thresholds)+1000))\n",
    "            node, edge = ox.graph_to_gdfs(graph)\n",
    "            road_node = pd.concat([road_node, node], axis = 0)\n",
    "            roads = pd.concat([roads, edge], axis = 0)\n",
    "        \n",
    "        # Eliminate lists in the df which prevents drop of duplicate columns\n",
    "        road_edge = pd.DataFrame([[c[0] if isinstance(c,list) else c for c in roads[col]]\\\n",
    "                              for col in roads]).transpose()\n",
    "        road_edge.columns = roads.columns\n",
    "        road_edge.index = roads.index\n",
    "        road_edge = gpd.GeoDataFrame(road_edge, crs = 4326)\n",
    "        \n",
    "        # Return the unique nodes and edges of the (often) adjacent OSM_areas.\n",
    "        road_node = road_node.drop_duplicates()\n",
    "        road_edge = road_edge.drop_duplicates()\n",
    "        \n",
    "        # Road nodes format\n",
    "        road_node = road_node.to_crs(4326)\n",
    "        road_node['geometry_m'] = gpd.GeoSeries(road_node['geometry'], crs = 4326).to_crs(3043)\n",
    "        road_node['osmid_var'] = road_node.index\n",
    "        road_node = gpd.GeoDataFrame(road_node, geometry = 'geometry', crs = 4326)\n",
    "\n",
    "        # format road edges\n",
    "        road_edge['geometry_m'] = gpd.GeoSeries(road_edge['geometry'], crs = 4326).to_crs(3043)\n",
    "        road_edge = road_edge.reset_index()\n",
    "        road_edge.rename(columns={'u':'from', 'v':'to', 'key':'keys'}, inplace=True)\n",
    "        road_edge['key'] = road_edge['from'].astype(str) + '-' + road_edge['to'].astype(str)\n",
    "        \n",
    "        if undirected == True:\n",
    "            # Apply one-directional to both for walking\n",
    "            both = road_edge[road_edge['oneway'] == False]\n",
    "            one = road_edge[road_edge['oneway'] == True]\n",
    "            rev = pd.DataFrame()\n",
    "            rev[['from','to']] = one[['to','from']]\n",
    "            rev = pd.concat([rev,one.iloc[:,2:]],axis = 1)\n",
    "            edge_bidir = pd.concat([both, one, rev])\n",
    "            edge_bidir = edge_bidir.reset_index()\n",
    "            edge_bidir['oneway'] = False\n",
    "        else:\n",
    "            edge_bidir = road_edge\n",
    "\n",
    "        # Exclude highways and ramps on edges    \n",
    "        edge_filter = edge_bidir[(edge_bidir['highway'].str.contains('motorway') | \n",
    "              (edge_bidir['highway'].str.contains('trunk') & \n",
    "               edge_bidir['maxspeed'].astype(str).str.contains(\n",
    "                   '40 mph|45 mph|50 mph|55 mph|60 mph|65|70|75|80|85|90|95|100|110|120|130|140'))) == False]\n",
    "        road_edges.append(edge_filter)\n",
    "\n",
    "        # Exclude isolated nodes\n",
    "        fltrnodes = pd.Series(list(edge_filter['from']) + list(edge_filter['to'])).unique()\n",
    "        newnodes = road_node[road_node['osmid_var'].isin(fltrnodes)]\n",
    "        road_nodes.append(newnodes)\n",
    "\n",
    "        # Get only necessary road connections columns for network performance\n",
    "        road_con = edge_filter[['osmid','key','length','geometry']]\n",
    "        road_con = road_con.set_index('key')\n",
    "\n",
    "        road_conn.append(road_con)\n",
    "\n",
    "        # formatting to graph again.\n",
    "        newnodes = newnodes.loc[:, ~newnodes.columns.isin(['geometry_m', 'osmid_var'])]\n",
    "        edge_filter = edge_filter.set_index(['from','to','keys'])\n",
    "        edge_filter = edge_filter.loc[:, ~edge_filter.columns.isin(['geometry_m', 'key'])]\n",
    "\n",
    "        graph2 = ox.graph_from_gdfs(newnodes, edge_filter)\n",
    "\n",
    "        graphs.append(graph2)\n",
    "        print(cities['City'][i[0]].rsplit(',')[0], 'done', round((time.time() - start_time) / 60,2),'mns')\n",
    "    return({'graphs':graphs,'nodes':road_nodes,'edges':road_conn,'edges long':road_edges})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de012d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 4 city greenspace\n",
    "def urban_greenspace (cities, thresholds, one_UGS_buf = 25, min_UGS_size = 400):\n",
    "    print('get urban greenspaces from OSM')\n",
    "    parks_in_range = list()\n",
    "    for i in enumerate(cities['OSM_area']):\n",
    "        # Tags seen as Urban Greenspace (UGS) require the following:\n",
    "        # 1. Tag represent an area\n",
    "        # 2. The area is outdoor\n",
    "        # 3. The area is (semi-)publically available\n",
    "        # 4. The area is likely to contain trees, grass and/or greenery\n",
    "        # 5. The area can reasonable be used for walking or recreational activities\n",
    "        tags = {'landuse':['allotments','forest','greenfield','village_green'],\\\n",
    "                'leisure':['garden','fitness_station','nature_reserve','park','playground'],\\\n",
    "                'natural':'grassland'}\n",
    "        gdf = ox.geometries_from_place(i[1].rsplit(', '),tags = tags,buffer_dist = np.max(thresholds))\n",
    "        gdf = gdf[(gdf.geom_type == 'Polygon') | (gdf.geom_type == 'MultiPolygon')]\n",
    "        greenspace = gdf.reset_index()    \n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "        green_buffer = gpd.GeoDataFrame(geometry = greenspace.to_crs(3043).buffer(one_UGS_buf).to_crs(4326))\n",
    "        greenspace['geometry_w_buffer'] = green_buffer\n",
    "        greenspace['geometry_w_buffer'] = gpd.GeoSeries(greenspace['geometry_w_buffer'], crs = 4326)\n",
    "        greenspace['geom buffer diff'] = greenspace['geometry_w_buffer'].difference(greenspace['geometry'])\n",
    "\n",
    "        # This function group components in itself that overlap (with the buffer set of 25 metres)\n",
    "        # https://stackoverflow.com/questions/68036051/geopandas-self-intersection-grouping\n",
    "        W = libpysal.weights.fuzzy_contiguity(greenspace['geometry_w_buffer'])\n",
    "        greenspace['components'] = W.component_labels\n",
    "        parks = greenspace.dissolve('components')\n",
    "\n",
    "        # Exclude parks below 0.04 ha.\n",
    "        parks = parks[parks.to_crs(3043).area > min_UGS_size]\n",
    "        print(cities['City'][i[0]], 'done')\n",
    "        parks = parks.reset_index()\n",
    "        parks['geometry_m'] = parks['geometry'].to_crs(3043)\n",
    "        parks['park_area'] = parks['geometry_m'].area\n",
    "        parks_in_range.append(parks)\n",
    "    return(parks_in_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06ac19c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5 park entry points\n",
    "def UGS_fake_entry(UGS, road_nodes, cities, UGS_entry_buf = 25, walk_radius = 500, entry_point_merge = 0):\n",
    "    print('get fake UGS entry points')\n",
    "    start_time = time.time()\n",
    "    ParkRoads = list()\n",
    "    for j in range(len(cities)):\n",
    "        ParkRoad = pd.DataFrame()\n",
    "        mat = list()\n",
    "        # For all\n",
    "        for i in range(len(UGS[j])):\n",
    "            dist = road_nodes[j]['geometry'].to_crs(3043).distance(UGS[j]['geometry'].to_crs(\n",
    "                3043)[i])\n",
    "            buf_nodes = road_nodes[j][(dist < UGS_entry_buf) & (dist > 0)]\n",
    "            mat.append(list(np.repeat(i, len(buf_nodes))))\n",
    "            ParkRoad = pd.concat([ParkRoad, buf_nodes])\n",
    "            if i % 100 == 0: print(cities[j].rsplit(',')[0], round(i/len(UGS[j])*100,1),'% done', \n",
    "                                  round((time.time() - start_time) / 60,2),' mns')\n",
    "        # Park no list conversion\n",
    "        mat_u = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, mat) for i in b]\n",
    "\n",
    "        # Format\n",
    "        ParkRoad['Park_No'] = mat_u\n",
    "        ParkRoad = ParkRoad.reset_index()\n",
    "        ParkRoad['park_lon'] = ParkRoad['geometry_m'].x\n",
    "        ParkRoad['park_lat'] = ParkRoad['geometry_m'].y\n",
    "        \n",
    "        # Get the road nodes intersecting with the parks' buffer\n",
    "        ParkRoad = pd.merge(ParkRoad, UGS[j][['geometry','park_area']], left_on = 'Park_No', right_index = True)\n",
    "\n",
    "        # Get the walkable park size\n",
    "        ParkRoad['park_size_walkable'] = ParkRoad['geometry_m'].buffer(walk_radius).to_crs(4326).intersection(ParkRoad['geometry_y'].to_crs(4326))\n",
    "        ParkRoad['walk_area'] = ParkRoad['park_size_walkable'].to_crs(3043).area\n",
    "        #ParkRoad['park_area'] = ParkRoad['geometry_y'].to_crs(3043).area\n",
    "        ParkRoad['share_walked'] = ParkRoad['walk_area'] / ParkRoad['park_area']\n",
    "                \n",
    "        # Merge fake UGS entry points if within X meters of each other for better system performance\n",
    "        # Standard no merging\n",
    "        ParkRoad = simplify_UGS_entry(ParkRoad, entry_point_merge = 0)\n",
    "                \n",
    "        ParkRoads.append(ParkRoad)\n",
    "\n",
    "        print(cities[j].rsplit(',')[0],'100 % done', \n",
    "                                  round((time.time() - start_time) / 60,2),' mns')\n",
    "    return(ParkRoads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c537f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5.5 (not in use, buffer is 0, thus retains all the park entry points as is)\n",
    "def simplify_UGS_entry(fake_UGS_entry, entry_point_merge = 0):\n",
    "    # Get buffer of nodes close to each other.\n",
    "    # Get the buffer\n",
    "    ParkComb = fake_UGS_entry\n",
    "    ParkComb['geometry_m_buffer'] = ParkComb['geometry_m'].buffer(entry_point_merge)\n",
    "\n",
    "    # Get and merge components\n",
    "    M = libpysal.weights.fuzzy_contiguity(ParkComb['geometry_m_buffer'])\n",
    "    ParkComb['components'] = M.component_labels\n",
    "\n",
    "    # Take centroid of merged components\n",
    "    centr = gpd.GeoDataFrame(ParkComb, geometry = 'geometry_x', crs = 4326).dissolve('components')['geometry_x'].centroid\n",
    "    centr = gpd.GeoDataFrame(centr)\n",
    "    centr.columns = ['comp_centroid']\n",
    "\n",
    "    # Get node closest to the centroid of all merged nodes, which accesses the road network.\n",
    "    ParkComb = pd.merge(ParkComb, centr, left_on = 'components', right_index = True)\n",
    "    ParkComb['centr_dist'] = ParkComb['geometry_x'].distance(ParkComb['comp_centroid'])\n",
    "    ParkComb = ParkComb.iloc[ParkComb.groupby('components')['centr_dist'].idxmin()]\n",
    "    return(ParkComb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e354607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 6 grid-parkentry combinations within euclidean threshold distance\n",
    "def suitible_combinations(UGS_entry, pop_grids, road_nodes, thresholds, cities, chunk_size = 10000000):\n",
    "    print('get potential (Euclidean) suitible combinations')\n",
    "    start_time = time.time()\n",
    "    RoadComb = list()\n",
    "    for l in range(len(cities)):\n",
    "        #blockA = block_combinations\n",
    "        print(cities[l])\n",
    "        len1 = len(pop_grids[l])\n",
    "        len2 = len(UGS_entry[l])\n",
    "\n",
    "        # Reduce the size of combinations per iteration\n",
    "        len4 = 1\n",
    "        len5 = len1 * len2\n",
    "        blockC = len5\n",
    "        while blockC > chunk_size:\n",
    "            blockC = len5 / len4\n",
    "            #print(blockC, len4)\n",
    "            len4 = len4+1\n",
    "\n",
    "        # Amount of grids taken per iteration block\n",
    "        block = round(len1 / len4)\n",
    "\n",
    "        output = pd.DataFrame()\n",
    "        # Checking all the combinations at once is too performance intensive, it is broken down per 1000 (or what you want)\n",
    "        for i in range(len4):\n",
    "            # Check all grid-park combinations per block\n",
    "            l1, l2 = range(i*block,(i+1)*block), range(0,len2)\n",
    "            listed = pd.DataFrame(list(product(l1, l2)))\n",
    "\n",
    "            # Merge grid and park information\n",
    "            grid_merged = pd.merge(listed, \n",
    "                                   pop_grids[l][['grid_lon','grid_lat','centroid','centroid_m']],\n",
    "                                   left_on = 0, right_index = True)\n",
    "            node_merged = pd.merge(grid_merged, \n",
    "                                   UGS_entry[l][['Park_No','osmid','geometry_x','geometry_y','geometry_m','park_lon','park_lat',\n",
    "                                       'share_walked','park_area','walk_area']], \n",
    "                                   left_on = 1, right_index = True)\n",
    "\n",
    "            # Preset index for merging\n",
    "            node_merged['key'] = range(0,len(node_merged))\n",
    "            node_merged = node_merged.set_index('key')\n",
    "            node_merged = node_merged.loc[:, ~node_merged.columns.isin(['index'])]\n",
    "\n",
    "            # Create lists for better computational performance\n",
    "            glon = list(node_merged['grid_lon'])\n",
    "            glat = list(node_merged['grid_lat'])\n",
    "            plon = list(node_merged['park_lon'])\n",
    "            plat = list(node_merged['park_lat'])\n",
    "\n",
    "            # Get the euclidean distances\n",
    "            mat = list()\n",
    "            for j in range(len(node_merged)):\n",
    "                mat.append(math.sqrt(abs(plon[j] - glon[j])**2 + abs(plat[j] - glat[j])**2))\n",
    "\n",
    "            # Check if distances are within 1000m and join remaining info and concat in master df per 1000.\n",
    "            mat_df = pd.DataFrame(mat)[(np.array(mat) <= np.max(thresholds))]\n",
    "\n",
    "            # join the other gravity euclidean scores and other information\n",
    "            mat_df.columns = ['Euclidean']    \n",
    "            mat_df = mat_df.join(node_merged)\n",
    "\n",
    "            output = pd.concat([output, mat_df])\n",
    "\n",
    "            print('in chunk',(i+1),'/',len4,len(mat_df),'suitible comb.')\n",
    "        # Renaming columns\n",
    "        print('total combinations within distance',len(output))\n",
    "\n",
    "        output.columns = ['Euclidean','Grid_No','Park_entry_No','grid_lon','grid_lat','Grid_coords_centroid','Grid_m_centroid',\n",
    "                      'Park_No','Parkroad_osmid','Park_geom','Parkroad_coords_centroid','Parkroad_m_centroid','park_lon',\n",
    "                      'park_lat','parkshare_walked','park_area','walk_area_m2']\n",
    "\n",
    "        output = output[['Euclidean','Grid_No','Park_entry_No','Grid_coords_centroid','Grid_m_centroid','walk_area_m2',\n",
    "                     'Park_No','Parkroad_osmid','Park_geom','Parkroad_coords_centroid','Parkroad_m_centroid','park_area']]\n",
    "\n",
    "        # Reinstate geographic elements\n",
    "        output = gpd.GeoDataFrame(output, geometry = 'Grid_coords_centroid', crs = 4326)\n",
    "        output['Grid_m_centroid'] = gpd.GeoSeries(output['Grid_m_centroid'], crs = 3043)\n",
    "        output['Parkroad_coords_centroid'] = gpd.GeoSeries(output['Parkroad_coords_centroid'], crs = 4326)\n",
    "        output['Parkroad_m_centroid'] = gpd.GeoSeries(output['Parkroad_m_centroid'], crs = 3043)\n",
    "\n",
    "        # Get the nearest entrance point for the grid centroids\n",
    "        output = gridroad_entry(output, road_nodes[l])\n",
    "\n",
    "        print('100 % gridentry done', round((time.time() - start_time) / 60,2),' mns')\n",
    "        RoadComb.append(output)\n",
    "    return (RoadComb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6058fb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridroad_entry (suitible_comb, road_nodes):    \n",
    "    start_time = time.time()\n",
    "    mat5 = list()\n",
    "    for i in range(len(suitible_comb)):\n",
    "        try:\n",
    "            nearest = int(road_nodes['geometry'].sindex.nearest(suitible_comb['Grid_coords_centroid'].iloc[i])[1])\n",
    "            mat5.append(road_nodes['osmid_var'].iloc[nearest])\n",
    "        except: \n",
    "            # sometimes two nodes are the exact same distance, then the first in the list is taken.\n",
    "            nearest = int(road_nodes['geometry'].sindex.nearest(suitible_comb['Grid_coords_centroid'].iloc[i])[1][0])\n",
    "            mat5.append(road_nodes['osmid_var'].iloc[nearest])\n",
    "        if i % 250000 == 0: print(round(i/len(suitible_comb)*100,1),'% gridentry done', round((time.time() - start_time) / 60,2),' mns')\n",
    "    # format resulting dataframe\n",
    "    suitible_comb['grid_osm'] = mat5\n",
    "    suitible_comb = pd.merge(suitible_comb, road_nodes['geometry'], left_on = 'grid_osm', right_index = True)\n",
    "    suitible_comb['geometry_m'] = gpd.GeoSeries(suitible_comb['geometry'], crs = 4326).to_crs(3043)\n",
    "    suitible_comb = suitible_comb.reset_index()\n",
    "    return(suitible_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f6944b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check grids in or out of UGS\n",
    "def grids_in_UGS (suitible_comb, UGS, pop_grid): \n",
    "    start_time = time.time()\n",
    "    RoadInOut = list()\n",
    "    for i in range(len(suitible_comb)):\n",
    "        UGS_geoms = UGS[i]['geometry'].to_crs(4326)\n",
    "        grid = pop_grid[i]['centroid']\n",
    "        lst = list()\n",
    "        print('Check grids within UGS')\n",
    "        for l in enumerate(UGS_geoms):\n",
    "            lst.append(grid.intersection(l[1]).is_empty == False)\n",
    "            if l[0] % 100 == 0: print(l[0], round((time.time() - start_time) / 60,2),' mns')\n",
    "\n",
    "        dfGrUGS = pd.DataFrame(pd.DataFrame(np.array(lst)).unstack())\n",
    "        dfGrUGS.columns = ['in_out_UGS']\n",
    "        merged = pd.merge(suitible_comb[i], dfGrUGS, left_on = ['Grid_No','Park_No'], right_index = True, how = 'left')\n",
    "        RoadInOut.append(merged)\n",
    "    return(RoadInOut)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d5b61e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 7 calculate route networks of all grid-parkentry combinations within euclidean threshold distance\n",
    "def route_finding (graphs, combinations, road_nodes, road_edges, cities, block_size = 250000, nn_iter = 10):\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    Routes = list()\n",
    "    Routes_detail = list()\n",
    "    for j in range(len(cities)):\n",
    "        Graph = graphs[j]\n",
    "        suit_raw = combinations[j] # iloc to test the iteration speed.\n",
    "        nodes = road_nodes[j]\n",
    "\n",
    "        In_UGS = suit_raw[suit_raw['in_out_UGS'] == True] # Check if a grid centroid is in an UGS\n",
    "        suitible = suit_raw[suit_raw['in_out_UGS'] == False].reset_index(drop = True) # recreate a subsequential index\n",
    "                                                                                      # for the other grids outside UGS\n",
    "        block = block_size # Execute with chunks for performance improvement.\n",
    "\n",
    "        Route_parts = pd.DataFrame()\n",
    "        Route_dparts = pd.DataFrame()\n",
    "        len2 = int(np.ceil(len(suitible)/block))\n",
    "        # Divide in chunks of block for computational load\n",
    "        for k in range(len2):    \n",
    "            suitible_chunk = suitible.iloc[k*block:k*block+block] # Select chunk\n",
    "\n",
    "            parknode = list(suitible_chunk['Parkroad_osmid'])\n",
    "            gridnode = list(suitible_chunk['grid_osm'])\n",
    "\n",
    "            s_mat = list([]) # origin (normally grid) osmid\n",
    "            s_mat1 = list([]) # destination (normally UGS) osmid\n",
    "            s_mat2 = list([]) # route id\n",
    "            s_mat3 = list([]) # step id\n",
    "            s_mat4 = list([]) # way calculated\n",
    "            s_mat5 = list([]) # way calculated id\n",
    "            mat_nn = [] # found nearest nodes by block\n",
    "            len1 = len(suitible_chunk)\n",
    "\n",
    "            print(cities[j].rsplit(',')[0], k+1,'/',len2,'range',k*block,'-',k*block+np.where(k*block+block >= len1,len1,block))\n",
    "            for i in range(len(suitible_chunk)):\n",
    "                try: \n",
    "                    # from grid to UGS.\n",
    "                    shortest = nx.shortest_path(Graph, gridnode[i], parknode[i], 'travel_dist', method = 'dijkstra')\n",
    "                    s_mat.append(shortest)\n",
    "                    shortest_to = list(shortest[1:len(shortest)])\n",
    "                    shortest_to.append(-1)\n",
    "                    s_mat1.append(shortest_to)\n",
    "                    s_mat2.append(list(np.repeat(i+block*k, len(shortest))))\n",
    "                    s_mat3.append(list(np.arange(0, len(shortest))))\n",
    "                    s_mat4.append('normal way')\n",
    "                    s_mat5.append(1)\n",
    "                except:\n",
    "                    try:\n",
    "                        # Check the reverse\n",
    "                        shortest = nx.shortest_path(Graph, parknode[i], gridnode[i], 'travel_dist', method = 'dijkstra')\n",
    "                        s_mat.append(shortest)\n",
    "                        shortest_to = list(shortest[1:len(shortest)])\n",
    "                        shortest_to.append(-1)\n",
    "                        s_mat1.append(shortest_to)\n",
    "                        s_mat2.append(list(np.repeat(i+block*k, len(shortest))))\n",
    "                        s_mat3.append(list(np.arange(0, len(shortest))))\n",
    "                        s_mat4.append('reverse way')\n",
    "                        s_mat5.append(0)\n",
    "                    except:\n",
    "                        # Otherwise find nearest nodes (grid and UGS) and try to find routes between them\n",
    "                        nn_route_finding(Graph, suitible_chunk, nodes, s_mat, s_mat1, s_mat2, s_mat3,\n",
    "                                             s_mat4, s_mat5, mat_nn, i, block, k, nn_iter)\n",
    "                        \n",
    "                if i % 10000 == 0: print(round((i+block*k)/len(suitible)*100,2),'% done',\n",
    "                                         round((time.time() - start_time) / 60,2),'mns')\n",
    "            print('for', len(mat_nn),'routes nearest nodes found')\n",
    "\n",
    "            print(round((i+block*k)/len(suitible)*100,2),'% pathfinding done', round((time.time() - start_time) / 60,2),'mns')\n",
    "\n",
    "            # Formats route information by route and step (detailed)\n",
    "            routes = route_formatting(s_mat, s_mat1, s_mat2, s_mat3, road_edges[j]) # Formats lists to routes detail.\n",
    "            print('formatting done', round((time.time() - start_time) / 60,2), 'mns')\n",
    "            \n",
    "            # Summarizes information by route\n",
    "            routes2 = route_summarization(routes, suitible_chunk, road_nodes[j], s_mat4, s_mat5) # formats routes to summary\n",
    "            print('dissolving done', round((time.time() - start_time) / 60,2), 'mns')\n",
    "            \n",
    "            Route_parts = pd.concat([Route_parts, routes2])\n",
    "            Route_dparts = pd.concat([Route_dparts, routes])\n",
    "\n",
    "        # Format grids in UGS to enable smooth df concat\n",
    "        In_UGS = In_UGS.set_geometry(In_UGS['Grid_coords_centroid'])\n",
    "        In_UGS = In_UGS[['geometry','Grid_No','grid_osm','Park_No','Park_entry_No','Parkroad_osmid',\n",
    "                                   'Grid_m_centroid','walk_area_m2',\n",
    "                                   'Euclidean','geometry_m']]\n",
    "\n",
    "        In_UGS['realG_osmid'] = suit_raw['Parkroad_osmid']\n",
    "        In_UGS['realP_osmid'] = suit_raw['grid_osm']\n",
    "        In_UGS['way_calc'] = 'grid in UGS'\n",
    "\n",
    "        Route_parts = pd.concat([Route_parts,In_UGS])\n",
    "        Route_parts = Route_parts.reset_index(drop = True)\n",
    "\n",
    "        Route_parts['gridpark_no'] = Route_parts['Grid_No'].astype(str) +'-'+ Route_parts['Park_No'].astype(str)\n",
    "\n",
    "        # All fill value 0 because no routes are calculated for grid centroids in UGSs\n",
    "        to_fill = ['way-id','route_cost','steps','real_G-entry','Tcost']                                   \n",
    "        Route_parts[to_fill] = Route_parts[to_fill].fillna(0)  \n",
    "            \n",
    "        Routes.append(Route_parts)\n",
    "        Routes_detail.append(Route_dparts)\n",
    "    return(Routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "854c5949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_route_finding(graph, suitible_chunk, nodes, mat_from, mat_to, mat_route, mat_step,\n",
    "                                             mat_way, mat_wbin, mat_nn, i, block, k, nn_iter):\n",
    "                        \n",
    "    # Order in route for nearest node:\n",
    "    # 1. gridnode to nearest to the original failed parknode\n",
    "    # 2. The reverse of 1.\n",
    "    # 3. nearest gridnode to the failed one and route to park\n",
    "    # 4. The reverse of 3.\n",
    "                        \n",
    "    gridosm = suitible_chunk['grid_osm'] # grid osmid\n",
    "    UGSosm = suitible_chunk['Parkroad_osmid'] # UGS osmid\n",
    "    nodeosm = nodes['osmid_var'] # road node osmid\n",
    "    nodegeom = nodes['geometry'] # road node geometry\n",
    "                        \n",
    "    len3 = 0\n",
    "    alt_route = list([])\n",
    "    while len3 < nn_iter and len(alt_route) < 1: # If a route is found (alt_route == 1) or until max iterations\n",
    "\n",
    "        len3 = len3 +1\n",
    "                            \n",
    "        nn = nn_finding(gridosm, UGSosm, nodeosm, nodegeom, nodes, i, len3) # finds nearest node.\n",
    "\n",
    "        nn_routing (graph, nn['currUGS'], nn['nearUGS'], nn['currgrid'], nn['neargrid'], \n",
    "                                        mat_way, mat_wbin, len3, alt_route) # executes route finding in try order.\n",
    "    if len(alt_route) == 0: \n",
    "        alt = alt_route \n",
    "    else: \n",
    "        alt = alt_route[0]\n",
    "    len4 = len(alt)\n",
    "    if len4 > 0: # If a route is found\n",
    "        mat_nn.append(i+block*k)\n",
    "        mat_from.append(alt)\n",
    "        shortest_to = list(alt[1:len(alt)])\n",
    "        shortest_to.append(-1)\n",
    "        mat_to.append(shortest_to)\n",
    "        mat_route.append(list(np.repeat(i+block*k,len4)))\n",
    "        mat_step.append(list(np.arange(0, len4)))\n",
    "    else: # If a route is not found\n",
    "        mat_from.append(-1)\n",
    "        mat_to.append(-1)\n",
    "        mat_route.append(i+block*k)\n",
    "        mat_step.append(-1)\n",
    "        mat_way.append('no way')\n",
    "        mat_wbin.append(2)\n",
    "        print('index',i+block*k,'No route')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ecd5703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_finding (gridosm, UGSosm, nodeosm, nodegeom, nodes, i, nn_i): \n",
    "    # Grid nearest\n",
    "    g_geom = nodegeom[nodeosm == int(gridosm[i:i+1])] # Get geom of current node UGS\n",
    "    g_nearest = pd.DataFrame((abs(float(g_geom.x) - nodegeom.x)**2 # Check distance UGS\n",
    "    +abs(float(g_geom.y) - nodegeom.y)**2)**(1/2)\n",
    "                            ).join(nodeosm).sort_values(0) # sort by distance ascending UGS\n",
    "\n",
    "    g_grid = g_nearest.iloc[nn_i,1] # get the nearest node according to the nn_iter UGS entry\n",
    "    g_park = list(UGSosm)[i] # current node\n",
    "        \n",
    "    p_geom = nodegeom[nodeosm == int(UGSosm[i:i+1])] # get the geom of the current node grid\n",
    "    p_nearest = pd.DataFrame((abs(float(p_geom.x) - nodegeom.x)**2 # Check distance grid\n",
    "    +abs(float(p_geom.y) - nodegeom.y)**2)**(1/2)\n",
    "                            ).join(nodeosm).sort_values(0) # sort by distance ascending grid\n",
    "\n",
    "    p_grid = list(gridosm)[i] # current node\n",
    "    p_park = p_nearest.iloc[nn_i,1] # get the nearest node to the nn_iter grid\n",
    "    return({'currUGS':p_grid, 'nearUGS':p_park,'currgrid':g_park, 'neargrid':g_grid})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50b89e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improve: 2-to-2 instead of 1-to-all.\n",
    "\n",
    "def nn_routing (graph, curr_UGS, near_UGS, curr_grid, near_grid, mat_way, mat_wbin, nn_i, found_route):\n",
    "    try:\n",
    "        found_route.append(nx.shortest_path(graph, curr_UGS, near_UGS, \n",
    "                                          'travel_dist', method = 'dijkstra'))\n",
    "        mat_way.append(str(nn_i)+'grid > n-park') # grid to nearest unseen UGS node\n",
    "        mat_wbin.append(1)\n",
    "    except:\n",
    "        try:\n",
    "            found_route.append(nx.shortest_path(graph, near_UGS, curr_UGS, \n",
    "                                              'travel_dist', method = 'dijkstra'))\n",
    "            mat_way.append(str(nn_i)+'n-park > grid') # nearest unseen UGS node to grid\n",
    "            mat_wbin.append(0)\n",
    "        except:\n",
    "            try:\n",
    "                found_route.append(nx.shortest_path(graph, curr_grid, near_grid, \n",
    "                                                  'travel_dist', method = 'dijkstra'))\n",
    "                mat_way.append(str(nn_i)+'n-grid > park') # nearest grid node to UGS\n",
    "                mat_wbin.append(1)\n",
    "            except:\n",
    "                try:\n",
    "                    found_route.append(nx.shortest_path(graph, near_grid, curr_grid, \n",
    "                                                      'travel_dist', method = 'dijkstra'))\n",
    "                    mat_way.append(str(nn_i)+'park > n-grid') # UGS to nearest grid node\n",
    "                    mat_wbin.append(0)\n",
    "                except:\n",
    "                    try:\n",
    "                        found_route.append(nx.shortest_path(graph, near_grid, near_UGS, \n",
    "                                                      'travel_dist', method = 'dijkstra'))\n",
    "                        mat_way.append(str(nn_i)+'park > n-grid') # UGS to nearest grid node\n",
    "                        mat_wbin.append(0)\n",
    "                    except:\n",
    "                        try:\n",
    "                            found_route.append(nx.shortest_path(graph, near_UGS, near_grid, \n",
    "                                                      'travel_dist', method = 'dijkstra'))\n",
    "                            mat_way.append(str(nn_i)+'park > n-grid') # UGS to nearest grid node\n",
    "                            mat_wbin.append(1)\n",
    "                        except:\n",
    "                            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cf68b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_formatting(mat_from, mat_to, mat_route, mat_step, road_edges):\n",
    "    # Unpack lists\n",
    "    s_mat_u = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, mat_from) for i in b]\n",
    "    s_mat_u1 = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, mat_to) for i in b]\n",
    "    s_mat_u2 = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, mat_route) for i in b]\n",
    "    s_mat_u3 = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, mat_step) for i in b]\n",
    "\n",
    "    # Format df\n",
    "    routes = pd.DataFrame([s_mat_u,s_mat_u1,s_mat_u2,s_mat_u3]).transpose()\n",
    "    routes.columns = ['from','to','route','step']\n",
    "    mat_key = list([])\n",
    "    for n in range(len(routes)): # get key of origin and destination\n",
    "        mat_key.append(str(int(s_mat_u[n])) + '-' + str(int(s_mat_u1[n])))\n",
    "    routes['key'] = mat_key\n",
    "    routes = routes.set_index('key')\n",
    "\n",
    "    # Add route information\n",
    "    routes = routes.join(road_edges, how = 'left') # to add road node information\n",
    "    routes = gpd.GeoDataFrame(routes, geometry = 'geometry', crs = 4326)\n",
    "    routes = routes.sort_values(by = ['route','step'])\n",
    "    return(routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e347555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_summarization(routes, suitible_comb, road_nodes, mat_way, mat_wbin):\n",
    "    # dissolve route\n",
    "    routes2 = routes[['route','geometry']].dissolve('route')\n",
    "\n",
    "    # get used grid- and parkosm. Differs at NN-route.\n",
    "    route_reset = routes.reset_index()\n",
    "    origin = route_reset['from'].iloc[list(route_reset.groupby('route')['step'].idxmin()),]\n",
    "    origin = origin.reset_index().iloc[:,-1]\n",
    "    dest = route_reset['from'].iloc[list(route_reset.groupby('route')['step'].idxmax()),]\n",
    "    dest = dest.reset_index().iloc[:,-1]\n",
    "\n",
    "    # grid > park = 1, park > grid = 0, no way = 2, detailed way in way_calc.\n",
    "    routes2['way-id'] = mat_wbin\n",
    "    routes2['realG_osmid'] = np.where(routes2['way-id'] == 1, origin, dest)\n",
    "    routes2['realP_osmid'] = np.where(routes2['way-id'] == 1, dest, origin)\n",
    "    routes2['way_calc'] = mat_way\n",
    "\n",
    "    # get route cost, steps, additional information.\n",
    "    routes2['route_cost'] = routes.groupby('route')['length'].sum()\n",
    "    routes2['steps'] = routes.groupby('route')['step'].max()\n",
    "    routes2['index'] = suitible_comb.index\n",
    "    routes2 = routes2.set_index(['index'])\n",
    "    routes2.index = routes2.index.astype(int)\n",
    "    routes2 = pd.merge(routes2, suitible_comb[['Grid_No','grid_osm','Park_No','Park_entry_No','Parkroad_osmid',\n",
    "                                          'Grid_m_centroid','walk_area_m2','Euclidean']],\n",
    "                                            left_index = True, right_index = True)\n",
    "    routes2 = pd.merge(routes2, road_nodes['geometry_m'], how = 'left', left_on = 'realG_osmid', right_index = True)\n",
    "    # calculate distance of used road-entry for grid-centroid.\n",
    "    routes2['real_G-entry'] = round(gpd.GeoSeries(routes2['Grid_m_centroid'], crs = 3043).distance(routes2['geometry_m']),3)\n",
    "                                    \n",
    "    # Calculcate total route cost for the four gravity variants\n",
    "    routes2['Tcost'] = routes2['route_cost'] + routes2['real_G-entry']\n",
    "    return(routes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ce9ee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_gridUGS_comb (routes, grids, UGS):\n",
    "    gp_nearest = []\n",
    "    for i in range(len(routes)):\n",
    "        gp_nn = routes[i][routes[i]['Tcost'] <= max(thresholds)]\n",
    "        gp_nn = pd.merge(gp_nn, grids[i]['population'], left_on='Grid_No', right_index = True)\n",
    "        gp_nn = pd.merge(gp_nn, UGS[i]['park_area'], left_on = 'Park_No', right_index = True)\n",
    "        gp_nn = gp_nn.reset_index()\n",
    "\n",
    "        gp_nn = gp_nn.iloc[gp_nn.groupby('gridpark_no')['Tcost'].idxmin()]\n",
    "        gp_nn.index.name = 'idx'\n",
    "        gp_nn = gp_nn.sort_values('idx')\n",
    "        gp_nn = gp_nn.reset_index()\n",
    "        gp_nearest.append(gp_nn)\n",
    "    gp_nearest[0].sort_values('Grid_No')\n",
    "    return(gp_nearest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3467e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def E2SCFA_scores(min_gridUGS_comb, grids, thresholds, cities, save_path = 'C:/Dumps/GEE-WP Scores/E2SFCA/', grid_size = 100):\n",
    "    pd.options.display.float_format = '{:20,.2f}'.format\n",
    "    E2SFCA_cities = []\n",
    "    E2SFCA_summary = pd.DataFrame()\n",
    "    for i in range(len(cities)):\n",
    "        E2SFCA_score = grids[i][['population','geometry']]\n",
    "        for j in range(len(thresholds)):\n",
    "            subset = min_gridUGS_comb[i][min_gridUGS_comb[i]['Tcost'] <= thresholds[j]]\n",
    "\n",
    "            # use gussian distribution: let v= 923325, then the weight for 800m is 0.5\n",
    "            v = -thresholds[j]**2/np.log(0.5)\n",
    "\n",
    "            # add a column of weight: apply the decay function on distance\n",
    "            subset['weight'] = np.exp(-(subset['Tcost']**2/v)).astype(float)\n",
    "            subset['pop_weight'] = subset['weight'] * subset['population']\n",
    "\n",
    "            # get the sum of weighted population each green space has to serve.\n",
    "            s_w_p = pd.DataFrame(subset.groupby('Park_No').sum('pop_weight')['pop_weight'])\n",
    "\n",
    "            # delete other columns, because they are useless after groupby\n",
    "            s_w_p = s_w_p.rename({'pop_weight':'pop_weight_sum'},axis = 1)\n",
    "            middle = pd.merge(subset,s_w_p, how = 'left', on = 'Park_No' )\n",
    "\n",
    "            # calculate the supply-demand ratio for each green space\n",
    "            middle['green_supply'] = middle['park_area']/middle['pop_weight_sum']\n",
    "\n",
    "            # caculate the accessbility score for each green space that each population grid cell could reach\n",
    "            middle['Sc-access'] = middle['weight'] * middle['green_supply']\n",
    "            # add the scores for each population grid cell\n",
    "            pop_score_df = pd.DataFrame(middle.groupby('Grid_No').sum('Sc-access')['Sc-access'])\n",
    "\n",
    "            # calculate the mean distance of all the green space each population grid cell could reach\n",
    "            mean_dist = middle.groupby('Grid_No').mean('Tcost')['Tcost']\n",
    "            pop_score_df['M-dist'] = mean_dist\n",
    "\n",
    "            # calculate the mean area of all the green space each population grid cell could reach\n",
    "            mean_area = middle.groupby('Grid_No').mean('park_area')['park_area']\n",
    "            pop_score_df['M-area'] = mean_area\n",
    "\n",
    "            # calculate the mean supply_demand ratio of all the green space each population grid cell could reach\n",
    "            mean_supply = middle.groupby('Grid_No').mean('green_supply')['green_supply']\n",
    "            pop_score_df['M-supply'] = mean_supply\n",
    "\n",
    "            pop_score = pop_score_df\n",
    "\n",
    "            pop_score_df = pop_score_df.join(grids[i]['population'], how = 'right')\n",
    "            pop_score_df['Sc-norm'] = pop_score_df['Sc-access'] / pop_score_df['population']\n",
    "\n",
    "            pop_score_df = pop_score_df.loc[:, pop_score_df.columns != 'population']\n",
    "            pop_score_df = pop_score_df.add_suffix(' '+str(thresholds[j]))\n",
    "            E2SFCA_score = E2SFCA_score.join(pop_score_df, how = 'left')\n",
    "\n",
    "            print(thresholds[j], cities[i])\n",
    "\n",
    "        E2SFCA_score = E2SFCA_score.fillna(0)\n",
    "        \n",
    "        if not os.path.exists(save_path+str(grid_size)+'m grids'+'/grid_geoms/'):\n",
    "            os.makedirs(save_path+str(grid_size)+'m grids'+'/grid_geoms/')\n",
    "        \n",
    "        E2SFCA_score.to_file(save_path+str(grid_size)+'m grids'+'/grid_geoms/'+cities[i]+'.gpkg') # Detailed scores\n",
    "        pop_sum = pd.Series(E2SFCA_score['population'].sum()).astype(int)\n",
    "        pop_sum.index = ['population']\n",
    "        mean_metrics = E2SFCA_score.loc[:, E2SFCA_score.columns != 'population'].mean()\n",
    "        E2SFCA_sum = pd.concat([pop_sum, mean_metrics])\n",
    "        E2SFCA_summary = pd.concat([E2SFCA_summary, E2SFCA_sum], axis = 1) # summarized results\n",
    "        E2SFCA_cities.append(E2SFCA_score)\n",
    "        \n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        \n",
    "        E2SFCA_score.loc[:, E2SFCA_score.columns != 'geometry'].to_csv(save_path+cities[i]+'.csv')\n",
    "    E2SFCA_summary.columns = cities\n",
    "    \n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    E2SFCA_summary.to_csv(save_path+str(grid_size)+'m grids'+'all_cities.csv')\n",
    "    E2SFCA_summary\n",
    "    return({'score summary':E2SFCA_summary,'score detail':E2SFCA_cities})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2146e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0710956a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
