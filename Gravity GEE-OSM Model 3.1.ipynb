{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71639573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system packages\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# non-geo numeric packages\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import product, combinations\n",
    "import pandas as pd\n",
    "\n",
    "# network and OSM packages\n",
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "city_geo = ox.geocoder.geocode_to_gdf\n",
    "\n",
    "# Earth engine packages\n",
    "import ee\n",
    "import geemap\n",
    "\n",
    "# General geo-packages\n",
    "import libpysal\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "from shapely import geometry\n",
    "from shapely.geometry import Point, MultiLineString, LineString, Polygon, MultiPolygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c23309aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=MFHS4rpz-fRl6xCsCdYJxGhHXqq9_kLyIY5frXD9Bnk&tc=f8VdAGGJLVwq9x5m4O6Dc3Ml-E1J2oYvo3600qMuv2U&cc=HXLYjbt3IB-H_wk3VBICwauM_wKWm5V2QKZwtH_hxqI>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=MFHS4rpz-fRl6xCsCdYJxGhHXqq9_kLyIY5frXD9Bnk&tc=f8VdAGGJLVwq9x5m4O6Dc3Ml-E1J2oYvo3600qMuv2U&cc=HXLYjbt3IB-H_wk3VBICwauM_wKWm5V2QKZwtH_hxqI</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you should paste in the box below.</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter verification code: 4/1AbUR2VPI5lq8Q2mGepfG0wsk97itcVLt0xXk4TCfgMHr8cUeYiiz0PQyCfo\n",
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "# Authenticate and Initialize Google Earth Engine\n",
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cc4fa99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Philippines']\n",
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/9d4d5da3465c7a87017061ee3117663d-0c972655acf74d4e32f34afe23bdb293:getPixels\n",
      "Please wait ...\n",
      "Data downloaded to D:\\Dumps\\GEE_city_grids\\PHL_Manila_2020.tif\n",
      "get road networks from OSM\n",
      "Manila done 2.58 mns\n",
      " \n",
      "get urban greenspaces from OSM\n",
      "Manila done\n",
      " \n",
      "100m resolution grids extraction\n",
      "Manila 3.44 mns\n",
      "\n",
      "Manila\n",
      "0.0 % fake entry points done 0.01  mns\n",
      "8.8 % fake entry points done 1.14  mns\n",
      "17.6 % fake entry points done 2.16  mns\n",
      "26.4 % fake entry points done 3.48  mns\n",
      "35.2 % fake entry points done 4.58  mns\n",
      "44.0 % fake entry points done 5.62  mns\n",
      "52.8 % fake entry points done 6.93  mns\n",
      "61.6 % fake entry points done 8.13  mns\n",
      "70.4 % fake entry points done 9.18  mns\n",
      "79.2 % fake entry points done 10.41  mns\n",
      "88.0 % fake entry points done 11.56  mns\n",
      "96.8 % fake entry points done 12.72  mns\n",
      "Manila 100 % fake entry points done 13.08  mns\n",
      "Manila 100 % done 13.12  mns\n",
      "\n",
      "get (Euclidean) suitible combinations\n",
      "0.0 % 0.0 mns\n",
      "13.09 % 0.46 mns\n",
      "26.18 % 0.96 mns\n",
      "39.27 % 1.5 mns\n",
      "52.36 % 2.01 mns\n",
      "65.45 % 2.55 mns\n",
      "78.53 % 3.1 mns\n",
      "91.62 % 3.7 mns\n",
      "100 % finding combinations done\n",
      "Manila 856663 suitible combinations\n",
      "\n",
      "obtain local graphs\n",
      "Manila\n",
      "0.0 % done 6.01 mns\n",
      "13.09 % done 6.51 mns\n",
      "26.18 % done 7.16 mns\n",
      "39.27 % done 7.83 mns\n",
      "52.36 % done 8.57 mns\n",
      "65.45 % done 9.41 mns\n",
      "78.53 % done 10.15 mns\n",
      "91.62 % done 10.95 mns\n",
      "100 % done 11.66 mns\n",
      "\n",
      "Manila\n",
      "0.0 % 0.34 mns\n",
      "3.6 % 0.96 mns\n",
      "7.2 % 1.59 mns\n",
      "10.8 % 2.2 mns\n",
      "14.4 % 2.8 mns\n",
      "18.0 % 3.42 mns\n",
      "21.6 % 4.14 mns\n",
      "25.2 % 4.75 mns\n",
      "28.8 % 5.4 mns\n",
      "32.4 % 6.1 mns\n",
      "36.0 % 6.75 mns\n",
      "39.6 % 7.39 mns\n",
      "43.2 % 8.08 mns\n",
      "46.8 % 8.69 mns\n",
      "50.4 % 9.38 mns\n",
      "54.0 % 10.09 mns\n",
      "57.6 % 10.72 mns\n",
      "61.2 % 11.34 mns\n",
      "64.8 % 12.0 mns\n",
      "68.4 % 12.65 mns\n",
      "72.0 % 13.27 mns\n",
      "75.6 % 13.86 mns\n",
      "79.2 % 14.49 mns\n",
      "82.8 % 15.17 mns\n",
      "86.4 % 15.85 mns\n",
      "90.0 % 16.54 mns\n",
      "93.6 % 17.27 mns\n",
      "97.2 % 17.91 mns\n",
      "100 % done 18.42 mns\n",
      "\n",
      "Manila\n",
      "entrance 0.2 mns\n",
      "grid  300\n",
      "grid  600\n",
      "grid  1000\n",
      "gravity**(1/2) 2.75 mns\n",
      "grid  300\n",
      "grid  600\n",
      "grid  1000\n",
      "gravity**(1/3) 6.33 mns\n",
      "grid  300\n",
      "grid  600\n",
      "grid  1000\n",
      "gravity**(1/5) 9.56 mns\n",
      "grid  300\n",
      "grid  600\n",
      "grid  1000\n",
      "Manila done 13.62 mns\n",
      "CPU times: total: 46min 37s\n",
      "Wall time: 1h 7min 12s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Manila</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">entrance_300</th>\n",
       "      <th>1 high</th>\n",
       "      <td>0.061143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2 medium</th>\n",
       "      <td>0.015303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 low</th>\n",
       "      <td>0.068714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4 no</th>\n",
       "      <td>0.854840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">entrance_600</th>\n",
       "      <th>1 high</th>\n",
       "      <td>0.082389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2 medium</th>\n",
       "      <td>0.085341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 low</th>\n",
       "      <td>0.206535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4 no</th>\n",
       "      <td>0.625735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">entrance_1000</th>\n",
       "      <th>1 high</th>\n",
       "      <td>0.173767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2 medium</th>\n",
       "      <td>0.181694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 low</th>\n",
       "      <td>0.223927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4 no</th>\n",
       "      <td>0.420613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">gravity**(1/2)_300</th>\n",
       "      <th>1 high</th>\n",
       "      <td>0.062273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2 medium</th>\n",
       "      <td>0.029740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 low</th>\n",
       "      <td>0.098046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4 no</th>\n",
       "      <td>0.809941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">gravity**(1/2)_600</th>\n",
       "      <th>1 high</th>\n",
       "      <td>0.094104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2 medium</th>\n",
       "      <td>0.130803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 low</th>\n",
       "      <td>0.280536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4 no</th>\n",
       "      <td>0.494557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">gravity**(1/2)_1000</th>\n",
       "      <th>1 high</th>\n",
       "      <td>0.231705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2 medium</th>\n",
       "      <td>0.247503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 low</th>\n",
       "      <td>0.237951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4 no</th>\n",
       "      <td>0.282841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">gravity**(1/3)_300</th>\n",
       "      <th>1 high</th>\n",
       "      <td>0.061171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2 medium</th>\n",
       "      <td>0.017830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 low</th>\n",
       "      <td>0.069553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4 no</th>\n",
       "      <td>0.851446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">gravity**(1/3)_600</th>\n",
       "      <th>1 high</th>\n",
       "      <td>0.079317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2 medium</th>\n",
       "      <td>0.088599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 low</th>\n",
       "      <td>0.221912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4 no</th>\n",
       "      <td>0.610172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">gravity**(1/3)_1000</th>\n",
       "      <th>1 high</th>\n",
       "      <td>0.174687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2 medium</th>\n",
       "      <td>0.209880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 low</th>\n",
       "      <td>0.283763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4 no</th>\n",
       "      <td>0.331670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">gravity**(1/5)_300</th>\n",
       "      <th>1 high</th>\n",
       "      <td>0.060791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2 medium</th>\n",
       "      <td>0.014040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 low</th>\n",
       "      <td>0.060503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4 no</th>\n",
       "      <td>0.864665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">gravity**(1/5)_600</th>\n",
       "      <th>1 high</th>\n",
       "      <td>0.075476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2 medium</th>\n",
       "      <td>0.077416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 low</th>\n",
       "      <td>0.198032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4 no</th>\n",
       "      <td>0.649076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">gravity**(1/5)_1000</th>\n",
       "      <th>1 high</th>\n",
       "      <td>0.156294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2 medium</th>\n",
       "      <td>0.189612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 low</th>\n",
       "      <td>0.269065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4 no</th>\n",
       "      <td>0.385029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "City                            Manila\n",
       "entrance_300        1 high    0.061143\n",
       "                    2 medium  0.015303\n",
       "                    3 low     0.068714\n",
       "                    4 no      0.854840\n",
       "entrance_600        1 high    0.082389\n",
       "                    2 medium  0.085341\n",
       "                    3 low     0.206535\n",
       "                    4 no      0.625735\n",
       "entrance_1000       1 high    0.173767\n",
       "                    2 medium  0.181694\n",
       "                    3 low     0.223927\n",
       "                    4 no      0.420613\n",
       "gravity**(1/2)_300  1 high    0.062273\n",
       "                    2 medium  0.029740\n",
       "                    3 low     0.098046\n",
       "                    4 no      0.809941\n",
       "gravity**(1/2)_600  1 high    0.094104\n",
       "                    2 medium  0.130803\n",
       "                    3 low     0.280536\n",
       "                    4 no      0.494557\n",
       "gravity**(1/2)_1000 1 high    0.231705\n",
       "                    2 medium  0.247503\n",
       "                    3 low     0.237951\n",
       "                    4 no      0.282841\n",
       "gravity**(1/3)_300  1 high    0.061171\n",
       "                    2 medium  0.017830\n",
       "                    3 low     0.069553\n",
       "                    4 no      0.851446\n",
       "gravity**(1/3)_600  1 high    0.079317\n",
       "                    2 medium  0.088599\n",
       "                    3 low     0.221912\n",
       "                    4 no      0.610172\n",
       "gravity**(1/3)_1000 1 high    0.174687\n",
       "                    2 medium  0.209880\n",
       "                    3 low     0.283763\n",
       "                    4 no      0.331670\n",
       "gravity**(1/5)_300  1 high    0.060791\n",
       "                    2 medium  0.014040\n",
       "                    3 low     0.060503\n",
       "                    4 no      0.864665\n",
       "gravity**(1/5)_600  1 high    0.075476\n",
       "                    2 medium  0.077416\n",
       "                    3 low     0.198032\n",
       "                    4 no      0.649076\n",
       "gravity**(1/5)_1000 1 high    0.156294\n",
       "                    2 medium  0.189612\n",
       "                    3 low     0.269065\n",
       "                    4 no      0.385029"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Thresholds and cities\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "thresholds = [300, 600, 1000] # route threshold in metres. WHO guideline speaks of access within 300m\n",
    "\n",
    "# Extract cities list\n",
    "iso = pd.read_excel('iso_countries.xlsx')\n",
    "cities = pd.read_excel('cities.xlsx')\n",
    "cities_adj = cities[cities['City'].isin(['Manila'])]\n",
    "cities_adj = cities_adj.reset_index()\n",
    "\n",
    "# 1. Required preprocess for information extraction\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Predifine in Excel: the (1) city name as \"City\" and (2) the OSM area that needs to be extracted as \"OSM_area\"\n",
    "# i.e. City = \"Los Angeles\" and OSM_area = \"Los Angeles county, Orange county CA\"\n",
    "files = gee_worldpop_extract(cities_adj,iso,'D:/Dumps/GEE_city_grids/')\n",
    "\n",
    "# Files are downloaded automatically to the specified path. Files are also stored in Google with a downloadlink:\n",
    "\n",
    "# 2. Information extraction\n",
    "\n",
    "# Get road networks\n",
    "road_networks = road_network(cities_adj, # Get 'all' (drive,walk,bike) network\n",
    "                              thresholds,\n",
    "                              undirected = True)\n",
    "print(' ')\n",
    "# Extract urban greenspace (UGS)\n",
    "UGS = urban_greenspace(cities_adj, \n",
    "                       thresholds,\n",
    "                       one_UGS_buf = 25, # buffer at which UGS is seen as one\n",
    "                       min_UGS_size = 400) # WHO sees this as minimum UGS size (400m2)\n",
    "\n",
    "print(' ')\n",
    "# Clip cities from countries, format population grids\n",
    "population_grids = city_grids_format(files,\n",
    "                                     cities_adj['OSM_area'],\n",
    "                                     road_networks['nodes'],\n",
    "                                     UGS,\n",
    "                                     grid_size = 100) # aggregating upwards to i.e. 200m, 300m etc. is possible\n",
    "print('')\n",
    "# Get fake entry points (between UGS and buffer limits)\n",
    "UGS_entry = UGS_fake_entry(UGS, \n",
    "                           road_networks['nodes'], \n",
    "                           road_networks['graphs'],\n",
    "                           cities_adj['City'],\n",
    "                           population_grids,\n",
    "                           thresholds,\n",
    "                           UGS_entry_buf = 25, # road nodes within 25 meters are seen as fake entry points\n",
    "                           walk_radius = 500, # assume that the average person only views a UGS up to 500m in radius\n",
    "                                                # more attractive\n",
    "                           entry_point_merge = 0) # merges closeby fake UGS entry points within X meters \n",
    "                                                    # what may be done for performance\n",
    "print('')\n",
    "suitible_enh = suitible_enhanced(UGS_entry, \n",
    "                                 population_grids, \n",
    "                                 road_networks['nodes'], \n",
    "                                 cities_adj['City'], \n",
    "                                 thresholds)\n",
    "print('')\n",
    "subgraphs = obtaining_subgraphs(road_networks['graphs'],\n",
    "                                population_grids,\n",
    "                                UGS_entry,\n",
    "                                road_networks['nodes'],\n",
    "                                cities_adj['City'],\n",
    "                                thresholds)\n",
    "print('')\n",
    "Dir_Routes = direct_routing (suitible_enh,\n",
    "                             subgraphs['graphs'],\n",
    "                             road_networks['edges'],\n",
    "                             cities_adj['City'],\n",
    "                            time_sleep = 30)\n",
    "\n",
    "print('')\n",
    "grid_scores = grid_score_summary (Dir_Routes, # Shortest routes by the Dijkstra algorithm, with gravity variant distance adj.\n",
    "                                  cities_adj['City'], \n",
    "                                  population_grids, \n",
    "                                  ext = '_Manila', # At multiple runs, the extention prevents the summarized file to be overwritten.\n",
    "                                  save_path = 'D:/Dumps/GEE-WP Scores/Gravity_adj/',\n",
    "                                  grid_size = 100) # Size of the grid in meters\n",
    "grid_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0bde4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gee_worldpop_extract (city_file, iso, save_path = None):\n",
    "    \n",
    "    cities = city_file\n",
    "    iso['name'] = np.where(iso['name'] == 'Macedonia','North Macedonia',iso['name'])\n",
    "    \n",
    "    # Get included city areas\n",
    "    OSM_incl = [cities[cities['City'] == city]['OSM_area'].tolist()[0].rsplit(', ') for city in cities['City'].tolist()]\n",
    "\n",
    "    # Get the city geoms\n",
    "    obj = [city_geo(city).dissolve()['geometry'].tolist()[0] for city in OSM_incl]\n",
    "\n",
    "    # Get the city countries\n",
    "    obj_displ = [city_geo(city).dissolve()['display_name'].tolist()[0].rsplit(', ')[-1]for city in OSM_incl]\n",
    "    print(obj_displ)\n",
    "    obj_displ = np.where(pd.Series(obj_displ).str.contains(\"Ivoire\"),\"CIte dIvoire\",obj_displ)\n",
    "\n",
    "    # Get the country's iso-code\n",
    "    iso_list = [iso[iso['name'] == ob]['alpha3'].tolist()[0] for ob in obj_displ]\n",
    "\n",
    "    # Based on the iso-code return the worldpop 2020\n",
    "    ee_worldpop = [ee.ImageCollection(\"WorldPop/GP/100m/pop\")\\\n",
    "        .filter(ee.Filter.date('2020'))\\\n",
    "        .filter(ee.Filter.inList('country', [io])).first() for io in iso_list]\n",
    "\n",
    "    # Clip the countries with the city geoms.\n",
    "    clipped = [ee_worldpop[i].clip(shapely.geometry.mapping(obj[i])) for i in range(0,len(obj))]\n",
    "\n",
    "    # Create path if non-existent\n",
    "    if save_path == None:\n",
    "        path = ''\n",
    "    else:\n",
    "        path = save_path\n",
    "        if not os.path.exists(path):\n",
    "                    os.makedirs(path)\n",
    "\n",
    "    # Export as TIFF file.\n",
    "    # Stored in form path + USA_Los Angeles_2020.tif\n",
    "    filenames = [path+iso_list[i]+'_'+cities['City'][i]+'_2020.tif' for i in range(len(obj))]\n",
    "    [geemap.ee_export_image(clipped[i], filename = filenames[i]) for i in range(0,len(obj))]\n",
    "    return(filenames)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # Block 2 Road networks\n",
    "def road_network (cities, thresholds, undirected = False):\n",
    "    print('get road networks from OSM')\n",
    "    start_time = time.time()\n",
    "    graphs = list()\n",
    "    road_nodes = list()\n",
    "    road_edges = list()\n",
    "    road_conn = list()\n",
    "\n",
    "    for i in enumerate(cities['OSM_area']):\n",
    "        # Get graph, road nodes and edges\n",
    "        road_node = pd.DataFrame()\n",
    "        roads = pd.DataFrame()\n",
    "        \n",
    "        # For each included OSM_area get the roads\n",
    "        for district in i[1].rsplit(', '):\n",
    "            graph = ox.graph_from_place(district, network_type = \"all\", buffer_dist = (np.max(thresholds)+1000))\n",
    "            node, edge = ox.graph_to_gdfs(graph)\n",
    "            road_node = pd.concat([road_node, node], axis = 0)\n",
    "            roads = pd.concat([roads, edge], axis = 0)\n",
    "        \n",
    "        # Eliminate lists in the df which prevents drop of duplicate columns\n",
    "        road_edge = pd.DataFrame([[c[0] if isinstance(c,list) else c for c in roads[col]]\\\n",
    "                              for col in roads]).transpose()\n",
    "        road_edge.columns = roads.columns\n",
    "        road_edge.index = roads.index\n",
    "        road_edge = gpd.GeoDataFrame(road_edge, crs = 4326)\n",
    "        \n",
    "        # Return the unique nodes and edges of the (often) adjacent OSM_areas.\n",
    "        road_node = road_node.drop_duplicates()\n",
    "        road_edge = road_edge.drop_duplicates()\n",
    "        \n",
    "        # Road nodes format\n",
    "        road_node = road_node.to_crs(4326)\n",
    "        road_node['geometry_m'] = gpd.GeoSeries(road_node['geometry'], crs = 4326).to_crs(3043)\n",
    "        road_node['osmid_var'] = road_node.index\n",
    "        road_node = gpd.GeoDataFrame(road_node, geometry = 'geometry', crs = 4326)\n",
    "\n",
    "        # format road edges\n",
    "        road_edge['geometry_m'] = gpd.GeoSeries(road_edge['geometry'], crs = 4326).to_crs(3043)\n",
    "        road_edge = road_edge.reset_index()\n",
    "        road_edge.rename(columns={'u':'from', 'v':'to', 'key':'keys'}, inplace=True)\n",
    "        road_edge['key'] = road_edge['from'].astype(str) + '-' + road_edge['to'].astype(str)\n",
    "        \n",
    "        if undirected == True:\n",
    "            # Apply one-directional to both for walking\n",
    "            both = road_edge[road_edge['oneway'] == False]\n",
    "            one = road_edge[road_edge['oneway'] == True]\n",
    "            rev = pd.DataFrame()\n",
    "            rev[['from','to']] = one[['to','from']]\n",
    "            rev = pd.concat([rev,one.iloc[:,2:]],axis = 1)\n",
    "            edge_bidir = pd.concat([both, one, rev])\n",
    "            edge_bidir = edge_bidir.reset_index()\n",
    "            edge_bidir['oneway'] = False\n",
    "        else:\n",
    "            edge_bidir = road_edge\n",
    "\n",
    "        # Exclude highways and ramps on edges    \n",
    "        edge_filter = edge_bidir[(edge_bidir['highway'].str.contains('motorway') | \n",
    "              (edge_bidir['highway'].str.contains('trunk') & \n",
    "               edge_bidir['maxspeed'].astype(str).str.contains(\n",
    "                   '40 mph|45 mph|50 mph|55 mph|60 mph|65|70|75|80|85|90|95|100|110|120|130|140'))) == False]\n",
    "        road_edges.append(edge_filter)\n",
    "\n",
    "        # Exclude isolated nodes\n",
    "        fltrnodes = pd.Series(list(edge_filter['from']) + list(edge_filter['to'])).unique()\n",
    "        newnodes = road_node[road_node['osmid_var'].isin(fltrnodes)]\n",
    "        road_nodes.append(newnodes)\n",
    "\n",
    "        # Get only necessary road connections columns for network performance\n",
    "        road_con = edge_filter[['osmid','key','length','geometry']]\n",
    "        road_con = road_con.set_index('key')\n",
    "\n",
    "        road_conn.append(road_con)\n",
    "\n",
    "        # formatting to graph again.\n",
    "        newnodes = newnodes.loc[:, ~newnodes.columns.isin(['geometry_m', 'osmid_var'])]\n",
    "        edge_filter = edge_filter.set_index(['from','to','keys'])\n",
    "        edge_filter = edge_filter.loc[:, ~edge_filter.columns.isin(['geometry_m', 'key'])]\n",
    "\n",
    "        graph2 = ox.graph_from_gdfs(newnodes, edge_filter)\n",
    "\n",
    "        graphs.append(graph2)\n",
    "        print(cities['City'][i[0]].rsplit(',')[0], 'done', round((time.time() - start_time) / 60,2),'mns')\n",
    "    return({'graphs':graphs,'nodes':road_nodes,'edges':road_conn,'edges long':road_edges})\n",
    "# Block 3 city greenspace\n",
    "def urban_greenspace (cities, thresholds, one_UGS_buf = 25, min_UGS_size = 400):\n",
    "    print('get urban greenspaces from OSM')\n",
    "    parks_in_range = list()\n",
    "    for i in enumerate(cities['OSM_area']):\n",
    "        # Tags seen as Urban Greenspace (UGS) require the following:\n",
    "        # 1. Tag represent an area\n",
    "        # 2. The area is outdoor\n",
    "        # 3. The area is (semi-)publically available\n",
    "        # 4. The area is likely to contain trees, grass and/or greenery\n",
    "        # 5. The area can reasonable be used for walking or recreational activities\n",
    "        tags = {'landuse':['allotments','forest','greenfield','village_green'],\\\n",
    "                'leisure':['garden','fitness_station','nature_reserve','park','playground'],\\\n",
    "                'natural':'grassland'}\n",
    "        gdf = ox.geometries_from_place(i[1].rsplit(', '),tags = tags,buffer_dist = np.max(thresholds))\n",
    "        gdf = gdf[(gdf.geom_type == 'Polygon') | (gdf.geom_type == 'MultiPolygon')]\n",
    "        greenspace = gdf.reset_index()    \n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "        green_buffer = gpd.GeoDataFrame(geometry = greenspace.to_crs(3043).buffer(one_UGS_buf).to_crs(4326))\n",
    "        greenspace['geometry_w_buffer'] = green_buffer\n",
    "        greenspace['geometry_w_buffer'] = gpd.GeoSeries(greenspace['geometry_w_buffer'], crs = 4326)\n",
    "        greenspace['geom buffer diff'] = greenspace['geometry_w_buffer'].difference(greenspace['geometry'])\n",
    "\n",
    "        # This function group components in itself that overlap (with the buffer set of 25 metres)\n",
    "        # https://stackoverflow.com/questions/68036051/geopandas-self-intersection-grouping\n",
    "        W = libpysal.weights.fuzzy_contiguity(greenspace['geometry_w_buffer'])\n",
    "        greenspace['components'] = W.component_labels\n",
    "        parks = greenspace.dissolve('components')\n",
    "\n",
    "        # Exclude parks below 0.04 ha.\n",
    "        parks = parks[parks.to_crs(3043).area > min_UGS_size]\n",
    "        print(cities['City'][i[0]], 'done')\n",
    "        parks = parks.reset_index()\n",
    "        parks['geometry_m'] = parks['geometry'].to_crs(3043)\n",
    "        parks['park_area'] = parks['geometry_m'].area\n",
    "        parks_in_range.append(parks)\n",
    "    return(parks_in_range)\n",
    "# Block 4 population grids extraction\n",
    "def city_grids_format(city_grids, cities_area, road_nodes, UGS, grid_size = 100):\n",
    "    start_time = time.time()\n",
    "    grids = []\n",
    "    print(str(grid_size) + 'm resolution grids extraction')\n",
    "    for i in range(len(city_grids)):\n",
    "        \n",
    "        # Open the raster file\n",
    "        with rasterio.open(city_grids[i]) as src:\n",
    "            band= src.read() # the population values\n",
    "            aff = src.transform # the raster bounds and size (affine)\n",
    "        \n",
    "        # Get the rowwise arrays, get a 2D dataframe\n",
    "        grid = pd.DataFrame()\n",
    "        for b in enumerate(band[0]):\n",
    "            grid = pd.concat([grid, pd.Series(b[1],name=b[0])],axis=1)\n",
    "        grid= grid.unstack().reset_index()\n",
    "        \n",
    "        # Unstack df to columns\n",
    "        grid.columns = ['row','col','value']\n",
    "        grid['minx'] = aff[2]+aff[0]*grid['col']\n",
    "        grid['miny'] = aff[5]+aff[4]*grid['row']\n",
    "        grid['maxx'] = aff[2]+aff[0]*grid['col']+aff[0]\n",
    "        grid['maxy'] = aff[5]+aff[4]*grid['row']+aff[4]\n",
    "        \n",
    "        # Create polygon from affine bounds and row/col indices\n",
    "        grid['geometry'] = [Polygon([(grid.minx[i],grid.miny[i]),\n",
    "                                   (grid.maxx[i],grid.miny[i]),\n",
    "                                   (grid.maxx[i],grid.maxy[i]),\n",
    "                                   (grid.minx[i],grid.maxy[i])])\\\n",
    "                          for i in range(len(grid))]\n",
    "        \n",
    "        # Set the df as geo-df\n",
    "        grid = gpd.GeoDataFrame(grid, crs = 4326) \n",
    "\n",
    "        # Get dissolvement_key for dissolvement. \n",
    "        grid['row3'] = np.floor(grid['row']/(grid_size/100)).astype(int)\n",
    "        grid['col3'] = np.floor(grid['col']/(grid_size/100)).astype(int)\n",
    "        grid['dissolve_key'] = grid['row3'].astype(str) +'-'+ grid['col3'].astype(str)\n",
    "        \n",
    "        # Define a city's OSM area as Polygon.\n",
    "        geo_ls = gpd.GeoSeries(city_geo(cities_area[i].split(', ')).dissolve().geometry)\n",
    "        \n",
    "        # Intersect grids with the city boundary Polygon.\n",
    "        insec = grid.intersection(geo_ls.tolist()[0])\n",
    "        \n",
    "        # Exclude grids outside the specified city boundaries\n",
    "        insec = insec[insec.area > 0]\n",
    "        \n",
    "        # Join in other information.\n",
    "        insec = gpd.GeoDataFrame(geometry = insec, crs = 4326).join(grid.loc[:, grid.columns != 'geometry'])\n",
    "        \n",
    "        # Dissolve into block by block grids\n",
    "        popgrid = insec[['dissolve_key','geometry','row3','col3']].dissolve('dissolve_key')\n",
    "        \n",
    "        # Get those grids populations and area. Only blocks with population and full blocks\n",
    "        popgrid['population'] = round(insec.groupby('dissolve_key')['value'].sum()).astype(int)\n",
    "        popgrid['area_m'] = round(gpd.GeoSeries(popgrid['geometry'], crs = 4326).to_crs(3043).area).astype(int)\n",
    "        popgrid = popgrid[popgrid['population'] > 0]\n",
    "        popgrid = popgrid[popgrid['area_m'] / popgrid['area_m'].max() > 0.95]\n",
    "\n",
    "        # Get centroids and coords\n",
    "        popgrid['centroid'] = popgrid['geometry'].centroid\n",
    "        popgrid['centroid_m'] = gpd.GeoSeries(popgrid['centroid'], crs = 4326).to_crs(3043)\n",
    "        popgrid['grid_lon'] = popgrid['centroid_m'].x\n",
    "        popgrid['grid_lat'] = popgrid['centroid_m'].y\n",
    "        popgrid = popgrid.reset_index()\n",
    "\n",
    "        minx = popgrid.bounds['minx']\n",
    "        maxx = popgrid.bounds['maxx']\n",
    "        miny = popgrid.bounds['miny']\n",
    "        maxy = popgrid.bounds['maxy']\n",
    "\n",
    "        # Some geometries result in a multipolygon when dissolving (like i.e. 0.05 meters), coords error.\n",
    "        # Therefore recreate the polygon.\n",
    "        Poly = []\n",
    "        for k in range(len(popgrid)):\n",
    "            Poly.append(Polygon([(minx[k],maxy[k]),(maxx[k],maxy[k]),(maxx[k],miny[k]),(minx[k],miny[k])]))\n",
    "        popgrid['geometry'] = Poly\n",
    "        \n",
    "        try:\n",
    "            entry_index = [int(road_nodes[i]['geometry'].sindex.nearest(grid)[1])\\\n",
    "                                 for grid in popgrid['centroid']]\n",
    "        except:\n",
    "            entry_index = [int(road_nodes[i]['geometry'].sindex.nearest(grid)[1][0])\\\n",
    "                                 for grid in popgrid['centroid']]\n",
    "            \n",
    "        nearest_index = road_nodes[i].iloc[entry_index]\n",
    "        popgrid['grid_osm'] = nearest_index.reset_index(drop = True)['osmid_var']\n",
    "        popgrid['node_geom'] = nearest_index.reset_index(drop = True)['geometry']\n",
    "        popgrid['node_geom_m'] = nearest_index.reset_index(drop = True)['geometry_m']\n",
    "        popgrid['G-entry cost'] = popgrid['node_geom_m'].distance(popgrid['centroid_m'])\n",
    "        \n",
    "        UGS_all = UGS[i].dissolve().geometry[0]\n",
    "        popgrid['in_out_UGS'] = popgrid.intersection(UGS_all).is_empty == False\n",
    "        \n",
    "        grids.append(popgrid)\n",
    "\n",
    "        print(city_grids[i].rsplit('_')[3], round((time.time() - start_time)/60,2),'mns')\n",
    "    return(grids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59bebc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5 park entry points\n",
    "def UGS_fake_entry(UGS, road_nodes, graphs, cities, pop_grids,\n",
    "                   thresholds, UGS_entry_buf = 25, walk_radius = 500, entry_point_merge = 0):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    ParkRoads = list()\n",
    "    for j in range(len(cities)):\n",
    "        ParkRoad = pd.DataFrame()\n",
    "        mat = list()\n",
    "        # For all\n",
    "        print(cities[j].rsplit(',')[0])\n",
    "        for i in range(len(UGS[j])):\n",
    "            dist = road_nodes[j]['geometry'].to_crs(3043).distance(UGS[j]['geometry'].to_crs(\n",
    "                3043)[i])\n",
    "            buf_nodes = road_nodes[j][(dist < UGS_entry_buf) & (dist > 0)]\n",
    "            mat.append(list(np.repeat(i, len(buf_nodes))))\n",
    "            ParkRoad = pd.concat([ParkRoad, buf_nodes])\n",
    "            if i % 100 == 0: print(round(i/len(UGS[j])*100,1),'% fake entry points done', \n",
    "                                  round((time.time() - start_time) / 60,2),' mns')\n",
    "                \n",
    "        print(cities[j].rsplit(',')[0],'100 % fake entry points done', round((time.time() - start_time) / 60,2),' mns')\n",
    "        \n",
    "        # Park no list conversion\n",
    "        mat_u = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, mat) for i in b]\n",
    "\n",
    "        # Format\n",
    "        ParkRoad['Park_No'] = mat_u\n",
    "        ParkRoad = ParkRoad.reset_index()\n",
    "        ParkRoad['park_lon'] = ParkRoad['geometry_m'].x\n",
    "        ParkRoad['park_lat'] = ParkRoad['geometry_m'].y\n",
    "        \n",
    "        # Get the road nodes intersecting with the parks' buffer\n",
    "        ParkRoad = pd.merge(ParkRoad, UGS[j][['geometry']], left_on = 'Park_No', right_index = True)\n",
    "\n",
    "        # Get the walkable park size\n",
    "        ParkRoad['park_size_walkable'] = ParkRoad['geometry_m'].buffer(walk_radius).to_crs(4326).intersection(ParkRoad['geometry_y'])\n",
    "        ParkRoad['walk_area'] = ParkRoad['park_size_walkable'].to_crs(3043).area\n",
    "        ParkRoad['park_area'] = ParkRoad['geometry_y'].to_crs(3043).area\n",
    "        ParkRoad['share_walked'] = ParkRoad['walk_area'] / ParkRoad['park_area']\n",
    "        \n",
    "        # Get size inflation factors for the gravity model\n",
    "        ParkRoad['size_infl_factor'] = ParkRoad['walk_area'] / ParkRoad['walk_area'].median()\n",
    "        ParkRoad['size_infl_sqr2'] = ParkRoad['size_infl_factor']**(1/2)\n",
    "        ParkRoad['size_infl_sqr3'] = ParkRoad['size_infl_factor']**(1/3)\n",
    "        ParkRoad['size_infl_sqr5'] = ParkRoad['size_infl_factor']**(1/5)\n",
    "        ParkRoad['raw'] = 1\n",
    "                \n",
    "        # Merge fake UGS entry points if within X meters of each other for better system performance\n",
    "        # Standard no merging\n",
    "        ParkRoad = simplify_UGS_entry(ParkRoad, entry_point_merge = 0)\n",
    "        ParkRoads.append(ParkRoad)\n",
    "        \n",
    "        print(cities[j].rsplit(',')[0],'100 % done', \n",
    "                                  round((time.time() - start_time) / 60,2),' mns')\n",
    "        \n",
    "    return(ParkRoads)\n",
    "# Block 5.5 (not in use, buffer is 0, thus retains all the park entry points as is)\n",
    "def simplify_UGS_entry(fake_UGS_entry, entry_point_merge = 0):\n",
    "    # Get buffer of nodes close to each other.\n",
    "    # Get the buffer\n",
    "    ParkComb = fake_UGS_entry\n",
    "    ParkComb['geometry_m_buffer'] = ParkComb['geometry_m'].buffer(entry_point_merge)\n",
    "\n",
    "    # Get and merge components\n",
    "    M = libpysal.weights.fuzzy_contiguity(ParkComb['geometry_m_buffer'])\n",
    "    ParkComb['components'] = M.component_labels\n",
    "\n",
    "    # Take centroid of merged components\n",
    "    centr = gpd.GeoDataFrame(ParkComb, geometry = 'geometry_x', crs = 4326).dissolve('components')['geometry_x'].centroid\n",
    "    centr = gpd.GeoDataFrame(centr)\n",
    "    centr.columns = ['comp_centroid']\n",
    "\n",
    "    # Get node closest to the centroid of all merged nodes, which accesses the road network.\n",
    "    ParkComb = pd.merge(ParkComb, centr, left_on = 'components', right_index = True)\n",
    "    ParkComb['centr_dist'] = ParkComb['geometry_x'].distance(ParkComb['comp_centroid'])\n",
    "    ParkComb = ParkComb.iloc[ParkComb.groupby('components')['centr_dist'].idxmin()]\n",
    "    return(ParkComb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "371c0f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suitible_enhanced (UGS_entry, pop_grids, road_nodes, cities, thresholds):\n",
    "    start_time = time.time()\n",
    "    suits_all = []\n",
    "    for j in range(len(cities)):\n",
    "        print('get (Euclidean) suitible combinations')\n",
    "        print('0.0 %', round((time.time() - start_time) / 60,2),'mns')\n",
    "        UGSe = UGS_entry[j]\n",
    "        entry_geoms = UGSe.geometry_m\n",
    "        pop = pop_grids[j]\n",
    "        road_node = road_nodes[j]\n",
    "\n",
    "        suits = pd.DataFrame()\n",
    "        cols = ['osmid','Park_No','walk_area','size_infl_sqr2','size_infl_sqr3','size_infl_sqr5']\n",
    "        for i in range(len(entry_geoms)):\n",
    "            max_infl = np.max(UGSe[['raw','size_infl_sqr2','size_infl_sqr3','size_infl_sqr5']], axis = 1)[i]\n",
    "            suit_df = pop[pop.node_geom_m.distance(entry_geoms.iloc[i]) < (max_infl*np.max(thresholds))]\n",
    "        \n",
    "            suit_df['UGSe_osmid_m'] = entry_geoms.iloc[i]\n",
    "            suit_df['Grid_No'] = suit_df.index\n",
    "            suit_df = suit_df[['Grid_No','grid_osm','G-entry cost','in_out_UGS','node_geom_m','UGSe_osmid_m']].reset_index(drop = True)\n",
    "            suit_df['Park_entry_No'] = UGSe.index[i]\n",
    "            #suit_df = pd.merge(suit_df, UGSe[cols], left_on = 'Park_entry_No',right_index = True, how = 'left')\n",
    "            suits = pd.concat([suits,suit_df])\n",
    "            if (i+1) % 500 == 0: print(round((i+1) / len(entry_geoms)*100,2),'%',\n",
    "                                       round((time.time() - start_time) / 60,2),'mns')\n",
    "            \n",
    "        suits = pd.merge(suits, UGSe[cols], left_on = 'Park_entry_No',right_index = True, how = 'left')\n",
    "        suits = suits.reset_index(drop = True)\n",
    "        suits = suits.rename(columns = {'osmid':'Parkroad_osmid','walk_area':'walk_area_m2'})\n",
    "        suits['gridpark_no'] = suits['Grid_No'].astype(str)+'-'+suits['Park_No'].astype(str)\n",
    "        suits['graph_key'] = suits['grid_osm'].astype(str)+'-'+suits['Parkroad_osmid'].astype(str)\n",
    "        suits_all.append(suits)\n",
    "        print('100 % finding combinations done')\n",
    "        print(cities[j],len(suits),'suitible combinations')\n",
    "    return(suits_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6553bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtaining_subgraphs(graphs, pop_grids, UGS_entry, nodes, cities, thresholds, time_sleep = 30):\n",
    "    print('obtain local graphs')\n",
    "    start_time = time.time()\n",
    "    subgraphs_all = []\n",
    "    suits_all = []\n",
    "    for j in range(len(cities)):\n",
    "        print(cities[j])\n",
    "        Graph = graphs[j]\n",
    "        pop = pop_grids[j]\n",
    "        UGSe = UGS_entry[j].sort_values('osmid')\n",
    "        road_node = nodes[j]\n",
    "        node_geoms = road_node.geometry_m\n",
    "        entry_geoms = UGSe.geometry_m\n",
    "        osmid = UGSe['osmid']\n",
    "        max_infl = np.max(UGSe[['raw','size_infl_sqr2','size_infl_sqr3','size_infl_sqr5']], axis = 1)*(np.max(thresholds))\n",
    "\n",
    "        dist = [node_geoms.distance(Point(i)) for i in entry_geoms]\n",
    "\n",
    "        print('0.0 % done',round((time.time() - start_time) / 60,2),'mns')\n",
    "        subgraphs = []\n",
    "        UGSe_ids = []\n",
    "        suits = pd.DataFrame()\n",
    "        for i in range(len(entry_geoms)):      \n",
    "            suit = road_node[['geometry_m']]\n",
    "            suit['UGSe_osmid_m'] = entry_geoms.iloc[i]\n",
    "            suit_df = dist[i]\n",
    "            suit_in = suit_df[suit_df <= max_infl.iloc[i]]\n",
    "            UGSe_ids.append(osmid.iloc[i])\n",
    "            suit_in = pd.DataFrame(suit_in).join(node_geoms)\n",
    "            suit_in['Parkroad_osmid'] = osmid.iloc[i]\n",
    "            subgraphs.append(Graph.subgraph(suit_in.index))\n",
    "            suits = pd.concat([suits, suit_in])\n",
    "\n",
    "            if (i+1) % 500 == 0: \n",
    "                print(round((i+1) / len(entry_geoms)*100,2),'% done',\n",
    "                                        round((time.time() - start_time) / 60,2),'mns')\n",
    "                time.sleep(time_sleep)\n",
    "        print('100 % done',round((time.time() - start_time) / 60,2),'mns')\n",
    "        subgraphs_all.append(pd.Series(subgraphs, index = UGSe_ids))\n",
    "        suits_all.append(suits)\n",
    "    return({'graphs':subgraphs_all,'graph nodes':suits_all})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeab83a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_fast (Geo_1, Geo_2):\n",
    "    return((abs(Geo_1.x - Geo_2.x)**2 + abs(Geo_1.y - Geo_2.y)**2).apply(math.sqrt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c670d949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_routing (suitible_comb, graphs, edges, cities, chunk = 20000, time_sleep = 15):\n",
    "    start_time = time.time()\n",
    "    Routes = []\n",
    "    Lines = []\n",
    "    for j in enumerate(cities):\n",
    "        print(j[1])\n",
    "        \n",
    "        suitible = suitible_comb[j[0]].sort_values('Parkroad_osmid').reset_index()\n",
    "        grouped = suitible[suitible['in_out_UGS'] == False].groupby(['Parkroad_osmid'])['grid_osm'].apply(list)\n",
    "        sets = grouped.apply(np.unique)\n",
    "\n",
    "        Conn = edges[j[0]]\n",
    "        SG = graphs[j[0]]\n",
    "        \n",
    "        SGr = SG.reset_index()\n",
    "        SG = SGr.iloc[pd.Series(SGr['index'].drop_duplicates()).index].set_index('index')[0]\n",
    "\n",
    "        num = int(np.ceil(chunk / sets.apply(len).mean()))\n",
    "        length = int(np.ceil(len(suitible['Parkroad_osmid'].unique())/num))\n",
    "\n",
    "        Routes_df = pd.DataFrame()\n",
    "        Lines_df = pd.DataFrame()\n",
    "        for l in range(length):\n",
    "            comb = suitible[suitible['Parkroad_osmid'].isin(sets.index[l*num:l*num+num])]\n",
    "            sets2 = sets[l*num:l*num+num]\n",
    "\n",
    "            parknode = list(comb['Parkroad_osmid'])\n",
    "            gridnode = list(comb['grid_osm'])\n",
    "            subgraph = SG[sets2.index]\n",
    "\n",
    "            ls = []\n",
    "            ls2 = []\n",
    "            ls3 = []\n",
    "            lod = []\n",
    "            lgk = []\n",
    "            Routes\n",
    "            for i in range(len(sets2)):\n",
    "                path = nx.single_source_dijkstra(subgraph.iloc[i], sets2.index[i], weight = 'length')\n",
    "\n",
    "                incl = np.isin(list(path[0].keys()),sets2.iloc[i])\n",
    "                incl2 = np.isin(list(path[1].keys()),sets2.iloc[i])\n",
    "\n",
    "                # route cost\n",
    "                orig_c = list(np.repeat(sets2.index[i],sum(incl)))\n",
    "                dest_c = list(np.array(list(path[0].keys()))[incl])\n",
    "                cost = list(np.array(list(path[0].values()))[incl])\n",
    "\n",
    "                ls = ls + orig_c\n",
    "                ls2= ls2+ dest_c\n",
    "                ls3= ls3+ cost\n",
    "\n",
    "                # route steps\n",
    "                orig_s = list(np.repeat(sets2.index[i],sum(incl2)))\n",
    "                dest_s = list(np.array(list(path[1].keys()))[incl2])\n",
    "                steps = list(np.array(list(path[1].values()),dtype=object)[incl2])\n",
    "\n",
    "                fr = []\n",
    "                to = []\n",
    "                og = []\n",
    "                de = []\n",
    "                for j in enumerate(steps):\n",
    "                    if len(j[1]) > 1:\n",
    "                        fr.append(j[1][:-1])\n",
    "                        to.append(j[1][1:])\n",
    "                        og.append(list(np.repeat(orig_s[j[0]], len(j[1][:-1]))))\n",
    "                        de.append(list(np.repeat(dest_s[j[0]], len(j[1][:-1]))))\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                fr = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, fr) for i in b]\n",
    "                to = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, to) for i in b]\n",
    "                og = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, og) for i in b]\n",
    "                de = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, de) for i in b]\n",
    "\n",
    "                gk = [str(fr[k])+'-'+str(to[k]) for k in range(len(to))]\n",
    "                gkr = [str(to[k])+'-'+str(fr[k]) for k in range(len(to))]\n",
    "                od = [str(de[k])+'-'+str(og[k]) for k in range(len(og))]\n",
    "\n",
    "                lgk.append(gk)\n",
    "                lod.append(od)\n",
    "\n",
    "            dist_df = pd.DataFrame({'UGSe_id':ls,'GrE_id':ls2,'route cost':ls3})\n",
    "            dist_df['graph_key'] = dist_df['GrE_id'].astype(str)+'-'+dist_df['UGSe_id'].astype(str)\n",
    "\n",
    "            routes = pd.merge(comb, dist_df, on = 'graph_key', how = 'left')\n",
    "            routes['route cost'] = np.where(routes['in_out_UGS'],0,routes['route cost'])\n",
    "            routes = routes[~routes['route cost'].isna()].reset_index(drop = True)\n",
    "\n",
    "            routes['G-entry cost'] = np.where(routes['in_out_UGS'],0,routes['G-entry cost'])\n",
    "\n",
    "            routes['raw_Tcost'] = routes['route cost']+routes['G-entry cost']\n",
    "            routes['grav2_Tcost'] = routes['raw_Tcost'] / routes['size_infl_sqr2']\n",
    "            routes['grav3_Tcost'] = routes['raw_Tcost'] / routes['size_infl_sqr3']\n",
    "            routes['grav5_Tcost'] = routes['raw_Tcost'] / routes['size_infl_sqr5']\n",
    "\n",
    "            lgk = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, lgk) for i in b]\n",
    "            lod = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, lod) for i in b]\n",
    "            \n",
    "            linestr = pd.DataFrame({'route no':lod,'route step':lgk})\n",
    "            \n",
    "            linestr = pd.merge(linestr, Conn.geometry, left_on = 'route step', right_index = True, how = 'left')\n",
    "            linestr = linestr[['route no','geometry']]\n",
    "            linestr = gpd.GeoDataFrame(linestr[['route no','geometry']], crs = 4326)\n",
    "            \n",
    "            linestr = linestr.dissolve('route no')\n",
    "            routes2 = pd.merge(routes, linestr, left_on = 'graph_key', right_index = True, how = 'left')\n",
    "            \n",
    "            Lines_df = pd.concat([Lines_df, linestr])\n",
    "            Routes_df = pd.concat([Routes_df, routes2])\n",
    "            \n",
    "            print(round(l*num / len(sets)*100,2),'%', \n",
    "                  round((time.time() - start_time) / 60,2),'mns')\n",
    "            time.sleep(time_sleep)\n",
    "        Routes_df = Routes_df.sort_values('index')\n",
    "        Routes_df = Routes_df.set_index('index')\n",
    "        Routes_df = Routes_df.reset_index(drop = True)\n",
    "        \n",
    "        Routes_df = Routes_df[Routes_df.columns[~Routes_df.columns.isin(['UGSe_id', 'GrE_id','size_infl_sqr2',\\\n",
    "                                                                  'size_infl_sqr3', 'size_infl_sqr5'])]]\n",
    "        \n",
    "        print('100 % done',round((time.time() - start_time) / 60,2),'mns')\n",
    "        \n",
    "        Routes.append(Routes_df)\n",
    "        Lines.append(Lines_df)\n",
    "    return(Routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93760a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 8 determine best parkentry points from each grid, then calculate grid scores\n",
    "# and finally aggregate city access in categories (high, medium, low and no access)\n",
    "def grid_score_summary (routes, cities, pop_grids, ext = '', grid_size = 100, save_path = 'C:/Dumps/GEE-WP Scores/Gravity/'):\n",
    "    start_time = time.time()\n",
    "    popg_acc = pd.DataFrame()\n",
    "    grid_scores = list([])\n",
    "    gridpark = list([])\n",
    "    for n in range(len(cities)):    \n",
    "        print(cities[n])\n",
    "\n",
    "        # For the four distance decay variants regarding park size.\n",
    "        l1 = list(['raw','grav2','grav3','grav5'])\n",
    "        m1 = list(['entrance','gravity**(1/2)','gravity**(1/3)','gravity**(1/5)'])\n",
    "        grid_score = list([])\n",
    "        gridparks = list([])\n",
    "        gridpark.append(gridparks)\n",
    "        popgrid_access = pd.DataFrame()\n",
    "        for i in range(len(l1)):\n",
    "            # Get the lowest indices grouped by a key consisting of grid no and park no (best entry point from a grid to a park)\n",
    "            var_best_routes = best_gridpark_comb (routes[n], l1[i], pop_grids[n])\n",
    "\n",
    "            grdsc = pd.DataFrame()\n",
    "            gridsc = pd.DataFrame()\n",
    "            print(m1[i], round((time.time() - start_time) / 60,2), 'mns')\n",
    "\n",
    "            # For each threshold given, calculate a score\n",
    "            for k in range(len(thresholds)):\n",
    "                \n",
    "                t = thresholds[k]\n",
    "                score = 'tr_'+ str(t)\n",
    "                scores = determine_scores(var_best_routes, pop_grids[n], thresholds[k], l1[i], cities[n], \n",
    "                                          save_path, grid_size = 100)\n",
    "                \n",
    "                grdsc = pd.concat([grdsc, scores['score_w_route']], axis = 1)\n",
    "                gridsc = pd.concat([gridsc, scores['grid_score']])\n",
    "                                \n",
    "                # Group according to the categories just created and sum the populations living in those grids\n",
    "                popgacc = pd.DataFrame()\n",
    "                popgacc[m1[i]+'_'+str(t)] = scores['score_w_route'].groupby(score+'_access')['population'].sum()\n",
    "                popgrid_access = pd.concat([popgrid_access, popgacc],axis=1)   \n",
    "\n",
    "                print('grid ',t)\n",
    "\n",
    "            grid_score.append(grdsc)\n",
    "\n",
    "            gridsc = gridsc.join(pop_grids[n]['geometry'])\n",
    "            gridsc = gpd.GeoDataFrame(gridsc, geometry = 'geometry', crs = 4326)\n",
    "\n",
    "            if not os.path.exists(save_path+str(grid_size)+'m grids/Grid_geoms/'):\n",
    "                os.makedirs(save_path+str(grid_size)+'m grids/Grid_geoms/')\n",
    "\n",
    "            gridsc.to_file(save_path+str(grid_size)+'m grids/Grid_geoms/gridscore_'+ l1[i] + '_' + cities[n] + '.gpkg')\n",
    "\n",
    "            # Detailed scores to files number of cities * ways to measure = number of files.\n",
    "            # Different threshold-scores are in the same dataframe\n",
    "            gridsc = gridsc.loc[:, gridsc.columns!='geometry']\n",
    "\n",
    "            if not os.path.exists(save_path+str(grid_size)+'m grids/Grid_csv/'):\n",
    "                os.makedirs(save_path+str(grid_size)+'m grids/Grid_csv/')\n",
    "\n",
    "            gridsc.to_csv(save_path+str(grid_size)+'m grids/Grid_csv/gridscore_'+ l1[i] + '_' + cities[n] + '.csv')\n",
    "            gridparks.append(var_best_routes)\n",
    "\n",
    "        grid_scores.append(grid_score)\n",
    "\n",
    "        # For each city, divide the population access by group by the total to get its share.\n",
    "        popgrid_access = popgrid_access / popgrid_access.sum()\n",
    "        popgrid_access = pd.DataFrame(popgrid_access.unstack())\n",
    "        popg_acc = pd.concat([popg_acc, popgrid_access], axis = 1)\n",
    "\n",
    "        print(cities[n],'done', round((time.time() - start_time) / 60,2), 'mns')\n",
    "    popg_acc.columns = cities\n",
    "    popg_acc.to_csv(save_path+str(grid_size)+'m grids/popgrid_access.csv')\n",
    "    return(popg_acc)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50bdbdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_gridpark_comb (routes, var_abbr, pop_grid):\n",
    "    str1 = 'gridpark_' + var_abbr\n",
    "    locals()[str1] = routes.iloc[routes.groupby('gridpark_no')[(str(var_abbr) +'_Tcost')].idxmin()]  \n",
    "\n",
    "    # Get grid information\n",
    "    locals()[str1] = pd.merge(locals()[str1], pop_grid[['population','geometry']],\n",
    "                            left_on = 'Grid_No', right_index = True, how = 'outer')\n",
    "    locals()[str1] = locals()[str1].reset_index()\n",
    "\n",
    "    # formatting\n",
    "    locals()[str1]['Park_No'] = locals()[str1]['Park_No'].fillna(-1)\n",
    "    locals()[str1]['Park_No'] = locals()[str1]['Park_No'].astype(int)\n",
    "    locals()[str1]['Park_entry_No'] = locals()[str1]['Park_entry_No'].fillna(-1)\n",
    "    locals()[str1]['Park_entry_No'] = locals()[str1]['Park_entry_No'].astype(int)\n",
    "    return(locals()[str1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28006c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_scores(var_df, pop_grid, thresholds, var_abbr, city, save_path, grid_size = 100):\n",
    "    t = thresholds\n",
    "    str2 = str(t)\n",
    "    score = 'tr_'+ str2\n",
    "\n",
    "    #Only get routes within the threshold given (it loops over every threshold) and calculate the scores\n",
    "    thold = var_df[var_df[var_abbr + '_Tcost'] <= t]\n",
    "    thold[score] = t - thold[var_abbr + '_Tcost']\n",
    "    thold['pop' + score] = thold[score] * thold['population']\n",
    "    thold['walk_area_ha' + str2] = var_df['walk_area_m2'] /10000\n",
    "    thold['walkha_person' + str2] = thold['population'] / thold['walk_area_ha' + str2]\n",
    "\n",
    "    # Join the gridpark information from before.\n",
    "    var_df = var_df.join(thold[[score,'pop' + score,'walk_area_ha' + str2, 'walkha_person' + str2]])\n",
    "    # get the grid_scores\n",
    "    gs = pd.DataFrame()\n",
    "    gs[[score,'pop_' + score,'walkha_' + str2]] = var_df.groupby(\n",
    "            'Grid_No')[score,'pop' + score, 'walk_area_ha' + str2].sum()\n",
    "\n",
    "    gs['walkha_person_' + score] = var_df.groupby('Grid_No')['walkha_person' + str2].mean()\n",
    "\n",
    "    trstr = var_df[var_df[score] > 0]\n",
    "    gs[score + '_parks'] = trstr.groupby('Grid_No')['gridpark_no'].count()\n",
    "\n",
    "    # Add the routes as a dissolved line_geom\n",
    "    gs[score + '_routes'] = gpd.GeoDataFrame(trstr[['Grid_No','geometry_x']],\n",
    "                                                  geometry = 'geometry_x', crs = 4326).dissolve('Grid_No')\n",
    "\n",
    "    # Add parks which grids have access to with its closest access point\n",
    "    gs[score+'Park:entry'] = trstr[trstr['Park_No'] >=0].groupby('Grid_No')['Park_No'].apply(list).astype(str\n",
    "    ) + ':' + trstr[trstr['Park_entry_No'] >=0].groupby('Grid_No')['Park_entry_No'].apply(list).astype(str)\n",
    "                \n",
    "    # determine the thresholds category-score. \n",
    "    # High >= threshold (perfect score to one park), medium is above half perfect, \n",
    "    # low is below this and no is no access to a park for a certain grid within the threshold given\n",
    "    gs[score+'_access'] = np.select([gs[score] >= t, (gs[score] < t) & (\n",
    "    gs[score]>= t/2), (gs[score] < t/2) & (gs[score]> 0), gs[score] <= 0],\n",
    "          ['1 high','2 medium','3 low','4 no'])\n",
    "    gs = gs.join(pop_grid['population'], how = 'outer')\n",
    "            \n",
    "    gs = gpd.GeoDataFrame(gs, geometry = score + '_routes', crs = 4326)\n",
    "            \n",
    "    if not os.path.exists(save_path+str(grid_size)+'m grids/Grid_lines/'):\n",
    "        os.makedirs(save_path+str(grid_size)+'m grids/Grid_lines/')\n",
    "                \n",
    "    gs.to_file(save_path+str(grid_size)+'m grids/Grid_lines/gridscore_'+ var_abbr + '_' + str2 + '_' + city + '.gpkg')\n",
    "            \n",
    "    gsc = gs.loc[:,~gs.columns.isin([score + '_routes'])]\n",
    "\n",
    "    return({'grid_score':gsc,'score_w_route':gs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a07b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1e1b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da28c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f67806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146ae178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
